<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SPECTER: Document-level Representation Learning using Citation-informed Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,78.63,123.05,73.39,10.75;1,152.02,121.52,1.70,6.99"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
							<email>armanc@allenai.org</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for Artificial Intelligence ‡ Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,171.83,123.05,82.26,10.75;1,254.09,121.52,1.70,6.99"><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
							<email>sergey@allenai.org</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for Artificial Intelligence ‡ Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.89,123.05,51.47,10.75;1,325.36,121.52,1.88,6.99"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
							<email>beltagy@allenai.org</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for Artificial Intelligence ‡ Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,345.16,123.05,71.28,10.75;1,416.44,121.52,1.88,6.99"><forename type="first">Doug</forename><surname>Downey</surname></persName>
							<email>dougd@allenai.org</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for Artificial Intelligence ‡ Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,436.24,123.05,75.28,10.75;1,511.52,121.52,1.98,6.99"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for Artificial Intelligence ‡ Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SPECTER: Document-level Representation Learning using Citation-informed Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.0-SNAPSHOT" ident="GROBID" when="2024-03-25T21:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,89.01,249.72,185.90,8.64;1,89.01,261.68,185.90,8.64;1,89.01,273.63,21.31,8.64">Representation learning is a critical ingredient for natural language processing systems.</s><s coords="1,123.37,273.63,151.54,8.64;1,89.01,285.59,185.90,8.64;1,89.01,297.54,185.90,8.64;1,88.65,309.50,186.26,8.64;1,89.01,321.45,184.25,8.64;1,89.01,333.41,184.42,8.64;1,89.01,345.36,151.03,8.64">Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token-and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.</s><s coords="1,245.29,345.36,29.62,8.64;1,89.01,357.32,184.25,8.64;1,89.01,369.27,185.90,8.64;1,89.01,381.23,184.25,8.64;1,89.01,393.18,22.42,8.64">For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.</s><s coords="1,114.49,393.18,158.77,8.64;1,89.01,405.14,185.90,8.64;1,89.01,417.09,185.90,8.64;1,89.01,429.05,184.25,8.64;1,89.01,441.00,184.25,8.64;1,89.01,452.96,25.18,8.64">We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.</s><s coords="1,124.15,452.96,149.11,8.64;1,89.01,464.91,184.26,8.64;1,89.01,476.87,184.25,8.64;1,89.01,488.82,46.22,8.64">Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning.</s><s coords="1,138.27,488.82,135.15,8.64;1,89.01,500.78,185.90,8.64;1,89.01,512.73,184.50,8.64;1,89.01,524.69,185.90,8.64;1,89.01,536.65,185.90,8.64;1,89.01,548.60,125.94,8.64">Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.</s><s coords="1,218.93,548.60,54.33,8.64;1,89.26,560.56,184.00,8.64;1,89.01,572.51,112.89,8.64;1,201.89,570.84,3.49,6.05">We show that SPECTER outperforms a variety of competitive baselines on the benchmark. 1</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" coords="1,72.00,595.45,82.81,10.75">Introduction</head><p><s coords="1,71.61,617.94,218.66,9.46;1,72.00,631.49,218.26,9.46;1,72.00,645.04,218.27,9.46;1,72.00,658.59,192.77,9.46">As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical.</s><s coords="1,268.17,658.59,23.91,9.46;1,72.00,672.14,218.26,9.46;1,72.00,685.69,220.08,9.46;1,72.00,699.24,218.27,9.46;1,72.00,712.79,50.79,9.46">In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) <ref type="bibr" coords="1,165.15,699.24,93.14,9.46" target="#b38">(Radford et al., 2018;</ref><ref type="bibr" coords="1,261.03,699.24,29.25,9.46;1,72.00,712.79,50.79,9.46" target="#b11">Devlin et al., 2019;</ref>.</s><s coords="1,125.52,712.79,22.16,9.46">While such models are widely used for representing individual words or sentences, extensions to whole-document embeddings are relatively underexplored.</s><s coords="1,150.42,712.79,23.61,9.46">Likewise, methods that do use inter-document signals to produce whole-document embeddings <ref type="bibr" coords="1,458.67,267.05,67.78,9.46" target="#b46">(Tu et al., 2017;</ref>) have yet to incorporate stateof-the-art pretrained LMs.</s><s coords="1,176.75,712.79,18.48,9.46">Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents.</s></p><p><s coords="1,318.19,334.79,209.17,9.46;1,307.28,348.34,218.27,9.46;1,307.28,361.89,218.27,9.46;1,305.83,375.44,219.72,9.46;1,307.28,388.99,218.26,9.46;1,307.28,402.54,218.27,9.46;1,307.28,415.90,195.93,9.64">A paper's title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an "off-the-shelf" pretrained language model-even a state-of-the-art model tailored to scientific text like the recent SciBERT <ref type="bibr" coords="1,399.09,402.54,97.72,9.46" target="#b3">(Beltagy et al., 2019)</ref>-does not result in accurate paper representations.</s><s coords="1,508.24,416.09,17.30,9.46;1,307.28,429.64,218.26,9.46;1,307.28,443.19,218.27,9.46;1,307.28,456.73,218.27,9.46;1,307.28,470.28,147.56,9.46">The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation.</s></p><p><s coords="1,318.19,483.83,207.55,9.46;1,307.28,497.38,218.26,9.46;1,307.28,510.93,94.01,9.46">In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents.</s><s coords="1,405.00,510.93,102.65,9.46;1,507.65,508.89,3.99,6.91;1,514.99,510.93,12.36,9.46;1,307.28,524.48,220.07,9.46;1,307.28,538.03,218.27,9.46;1,306.92,551.58,218.63,9.46;1,307.28,565.13,218.26,9.46;1,307.28,578.68,218.27,9.46;1,307.28,592.23,220.07,9.46;1,307.28,605.78,107.50,9.46">Our system, SPECTER, 2 incorporates inter-document context into the Transformer <ref type="bibr" coords="1,342.02,538.03,102.29,9.46" target="#b47">(Vaswani et al., 2017)</ref> language models (e.g., SciBERT <ref type="bibr" coords="1,381.41,551.58,98.84,9.46" target="#b3">(Beltagy et al., 2019)</ref>) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model.</s><s coords="1,418.74,605.78,108.61,9.46;1,307.28,619.32,220.08,9.46;1,307.28,632.87,220.08,9.46;1,307.28,646.42,218.27,9.46;1,307.28,659.97,154.88,9.46">We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective.</s><s coords="1,467.73,659.97,58.20,9.46;1,307.28,673.52,218.27,9.46;1,307.28,687.07,146.93,9.46">Unlike many prior works, at inference time, our model does not require any citation information.</s><s coords="1,460.23,687.07,65.32,9.46;1,307.28,700.62,218.27,9.46;1,307.28,714.17,24.41,9.46">This is critical for embedding new papers that have not yet been cited.</s><s coords="1,337.64,714.17,187.90,9.46;1,307.28,727.72,220.08,9.46;2,72.00,66.67,220.07,9.46;2,72.00,80.22,218.27,9.46;2,72.00,93.76,76.05,9.46">In experiments, we show that SPECTER's representations substantially outperform the state-of-the-art on a variety of document-level tasks, including topic classification, citation prediction, and recommendation.</s></p><p><s coords="2,82.91,110.48,209.17,9.46;2,72.00,124.03,128.87,9.46;2,201.13,121.99,3.99,6.91;2,208.34,124.03,81.93,9.46;2,72.00,137.58,220.08,9.46;2,72.00,151.13,192.68,9.46">As an additional contribution of this work, we introduce and release SCIDOCS 3 , a novel collection of data sets and an evaluation suite for documentlevel embeddings in the scientific domain.</s><s coords="2,271.62,151.13,20.45,9.46;2,72.27,164.68,219.81,9.46;2,72.00,178.23,218.27,9.46;2,72.00,191.78,95.33,9.46">SCI-DOCS covers seven tasks, and includes tens of thousands of examples of anonymized user signals of document relatedness.</s><s coords="2,170.71,191.78,119.56,9.46;2,72.00,205.33,218.27,9.46;2,72.00,218.88,218.27,9.46;2,72.00,232.43,154.52,9.46">We also release our training set (hundreds of thousands of paper titles, abstracts and citations), along with our trained embedding model and its associated code base.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" coords="2,72.00,266.03,50.47,10.75;2,72.00,295.52,69.16,9.81">Model 2.1 Overview</head><p><s coords="2,72.00,319.72,220.08,9.46;2,72.00,333.27,114.73,9.46">Our goal is to learn task-independent representations of academic papers.</s><s coords="2,191.92,333.27,98.35,9.46;2,72.00,346.82,218.27,9.46;2,72.00,360.37,218.45,9.46;2,72.00,373.91,218.27,9.46;2,72.00,387.46,26.89,9.46">Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper.</s><s coords="2,102.46,387.46,187.82,9.46;2,72.00,401.01,220.08,9.46;2,72.00,414.56,218.27,9.46;2,72.00,428.11,220.17,9.46">Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information.</s><s coords="2,71.66,441.66,218.61,9.46;2,72.00,455.21,70.15,9.46">This limits their ability to learn optimal document representations.</s><s coords="2,146.85,455.21,145.22,9.46;2,72.00,468.76,218.27,9.46;2,72.00,482.31,220.07,9.46;2,72.00,495.86,178.53,9.46">To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective.</s><s coords="2,253.92,495.86,36.35,9.46;2,72.00,509.41,218.27,9.46;2,72.00,522.96,220.07,9.46;2,72.00,536.50,218.27,9.46;2,72.00,550.05,200.61,9.46">We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not.</s><s coords="2,276.00,550.05,14.27,9.46;2,72.00,563.60,218.27,9.46;2,72.00,577.15,220.08,9.46;2,72.00,590.70,41.78,9.46">We call our model SPECTER, which learns Scientific Paper Embeddings using Citation-informed Trans-formERs.</s><s coords="2,117.18,590.70,173.46,9.46;2,72.00,604.25,218.27,9.46;2,72.00,617.80,220.08,9.46;2,72.00,631.35,218.27,9.46;2,72.00,644.90,218.27,9.46;2,72.00,658.45,219.63,9.46;2,71.61,672.00,220.56,9.46">With respect to the terminology used by <ref type="bibr" coords="2,72.00,604.25,82.22,9.46" target="#b11">Devlin et al. (2019)</ref>, unlike most existing LMs that are "fine-tuning based", our approach results in embeddings that can be applied to downstream tasks in a "feature-based" fashion, meaning the learned paper embeddings can be easily used as features, with no need for further task-specific fine-tuning.</s><s coords="2,72.00,685.55,218.27,9.46;2,72.00,699.09,218.27,9.46;2,72.00,712.64,218.27,9.46;2,72.00,726.19,88.08,9.46">In the following, as background information, we briefly describe how pretrained LMs can be applied for document representation and then discuss the details of SPECTER.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2" coords="2,307.28,192.74,205.53,9.81">Background: Pretrained Transformers</head><p><s coords="2,307.28,212.06,218.27,9.46;2,307.28,225.60,220.07,9.46;2,307.28,239.15,165.25,9.46">Recently, pretrained Transformer networks have demonstrated success on various NLP tasks <ref type="bibr" coords="2,502.24,225.60,20.09,9.46;2,307.28,239.15,75.26,9.46" target="#b38">(Radford et al., 2018;</ref><ref type="bibr" coords="2,385.78,239.15,86.75,9.46" target="#b11">Devlin et al., 2019;</ref><ref type="bibr" coords="2,336.42,252.70,72.68,9.46" target="#b33">Liu et al., 2019)</ref>; we use these models as the foundation for SPECTER.</s><s coords="2,475.77,239.15,51.14,9.46;2,307.28,252.70,20.28,9.46">Specifically, we use SciBERT <ref type="bibr" coords="2,352.07,279.80,94.28,9.46" target="#b3">(Beltagy et al., 2019)</ref> which is an adaptation of the original BERT <ref type="bibr" coords="2,434.81,293.35,91.47,9.46" target="#b11">(Devlin et al., 2019)</ref> architecture to the scientific domain.</s><s coords="2,327.55,252.70,5.07,9.46">The BERT model architecture <ref type="bibr" coords="2,390.37,320.45,86.74,9.46" target="#b11">(Devlin et al., 2019)</ref> uses multiple layers of Transformers <ref type="bibr" coords="2,421.91,334.00,92.83,9.46" target="#b47">(Vaswani et al., 2017)</ref> to encode the tokens in a given input sequence.</s><s coords="2,336.42,252.70,15.46,9.46">Each layer consists of a self-attention sublayer followed by a feedforward sublayer.</s><s coords="2,355.67,252.70,22.03,9.46">The final hidden state associated with the special [CLS] token is usually called the "pooled output", and is commonly used as an aggregate representation of the sequence.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="2,307.28,438.25,121.30,9.81">Document Representation</head><p><s coords="2,439.49,438.68,87.86,9.46;2,307.28,452.22,79.77,9.46;2,389.77,450.87,7.59,18.93;2,400.98,451.84,124.56,9.84;2,307.28,465.77,218.27,9.46;2,307.28,479.32,25.01,9.46">Our goal is to represent a given paper P as a dense vector v that best represents the paper and can be used in downstream tasks.</s><s coords="2,335.94,479.32,189.61,9.46;2,307.28,492.87,103.89,9.46">SPECTER builds embeddings from the title and abstract of a paper.</s><s coords="2,415.46,492.87,111.90,9.46;2,307.28,506.42,218.27,9.46;2,307.28,519.97,220.08,9.46;2,307.28,533.52,213.79,9.46;2,521.06,531.47,3.99,6.91">Intuitively, we would expect these fields to be sufficient to produce accurate embeddings, since they are written to provide a succinct and comprehensive summary of the paper. <ref type="bibr" coords="2,521.06,531.47,3.99,6.91">4</ref></s><s coords="2,306.88,547.07,220.47,9.46;2,307.28,560.62,218.27,9.46;2,307.28,574.17,218.27,9.46;2,307.28,587.72,166.33,9.46">s such, we encode the concatenated title and abstract using a Transformer LM (e.g., SciBERT) and take the final representation of the [CLS] token as the output representation of the paper:</s></p><formula xml:id="formula_0" coords="2,335.41,585.67,190.13,27.50">5 v = Transformer(input) [CLS] , (1)</formula><p><s coords="2,306.88,622.50,220.47,9.46;2,306.88,636.05,218.66,9.46;2,307.28,649.60,218.99,9.46;2,307.28,663.15,218.65,9.46;3,72.00,66.67,78.86,9.46">where Transformer is the Transformer's forward function, and input is the concatenation of the [CLS] token and WordPieces <ref type="bibr" coords="2,454.91,649.60,71.36,9.46" target="#b52">(Wu et al., 2016)</ref> of the title and abstract of a paper, separated by the [SEP] token.</s><s coords="3,154.26,66.67,136.00,9.46;3,72.00,80.22,219.63,9.46;3,72.00,93.76,220.07,9.46;3,72.00,107.31,189.94,9.46">We use SciBERT as our model initialization as it is optimized for scientific text, though our formulation is general and any Transformer language model instead of SciBERT.</s><s coords="3,264.65,107.31,25.62,9.46;3,72.00,120.86,218.61,9.46;3,72.00,134.41,218.27,9.46;3,72.00,147.96,56.93,9.46">Using the above method with an "off-the-shelf" SciBERT does not take global inter-document information into account.</s><s coords="3,132.32,147.96,158.13,9.46;3,72.00,161.51,218.27,9.46;3,72.00,175.06,218.45,9.46;3,72.00,188.61,218.27,9.46;3,72.00,202.16,35.10,9.46">This is because SciBERT, like other pretrained language models, is trained via language modeling objectives, which only predict words or sentences given their in-document, nearby textual context.</s><s coords="3,110.47,202.16,181.61,9.46;3,72.00,215.71,218.27,9.46;3,72.00,229.26,220.07,9.46;3,72.00,242.81,162.12,9.46">In contrast, we propose to incorporate citations into the model as a signal of inter-document relatedness, while still leveraging the model's existing strength in modeling language.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3" coords="3,72.00,266.07,198.05,9.81">Citation-Based Pretraining Objective</head><p><s coords="3,71.61,284.72,218.66,9.46;3,72.00,298.27,130.10,9.46">A citation from one document to another suggests that the documents are related.</s><s coords="3,205.39,298.27,86.69,9.46;3,72.00,311.82,218.27,9.46;3,72.00,325.37,218.27,9.46;3,72.00,338.92,218.26,9.46;3,72.00,352.47,218.27,9.46;3,72.00,366.02,44.24,9.46">To encode this relatedness signal into our representations, we design a loss function that trains the Transformer model to learn closer representations for papers when one cites the other, and more distant representations otherwise.</s><s coords="3,119.61,366.02,170.66,9.46;3,72.00,379.56,81.54,9.46">The high-level overview of the model is shown in Figure <ref type="figure" coords="3,145.36,379.56,4.09,9.46" target="#fig_0">1</ref>.</s></p><p><s coords="3,82.91,393.49,207.36,9.46;3,72.00,407.04,91.98,9.46;3,166.48,405.69,7.59,18.93;3,174.97,404.74,6.66,6.99;3,182.13,407.04,72.13,9.46;3,256.76,405.69,7.59,18.93;3,265.24,404.74,6.59,6.99;3,274.83,407.04,15.44,9.46;3,72.00,420.59,70.06,9.46;3,144.79,419.24,7.59,18.93;3,153.28,418.29,6.59,6.99;3,160.36,420.59,2.69,9.46">In particular, each training instance is a triplet of papers: a query paper P Q , a positive paper P + and a negative paper P − .</s><s coords="3,166.46,420.59,123.99,9.46;3,72.00,434.14,218.45,9.46;3,72.00,447.69,218.27,9.46;3,72.00,461.24,87.25,9.46;3,161.98,459.88,7.59,18.93;3,170.46,458.94,6.59,6.99;3,177.55,461.24,6.26,9.46">The positive paper is a paper that the query paper cites, and the negative paper is a paper that is not cited by the query paper (but that may be cited by P + ).</s><s coords="3,187.20,461.24,103.07,9.46;3,72.00,474.79,208.62,9.46">We then train the model using the following triplet margin loss function:</s></p><formula xml:id="formula_1" coords="3,76.61,491.39,213.66,10.33">L = max d P Q , P + − d P Q , P − + m , 0 (2)</formula><p><s coords="3,71.61,512.98,218.66,9.81;3,72.00,526.87,218.27,9.46;3,72.00,540.08,35.94,9.57">where d is a distance function and m is the loss margin hyperparameter (we empirically choose m = 1).</s><s coords="3,111.32,540.42,154.81,9.46">Here, we use the L2 norm distance:</s></p><formula xml:id="formula_2" coords="3,118.94,553.12,124.40,20.42">d(P A , P B ) = v A − v B 2 ,</formula><p><s coords="3,71.61,574.49,218.66,10.72;3,72.00,588.07,218.27,9.81;3,71.18,601.97,11.80,9.46;3,82.98,599.93,3.99,6.91">where v A is the vector corresponding to the pooled output of the Transformer run on paper A (Equation <ref type="formula" coords="3,71.18,601.97,3.93,9.46">1</ref>). <ref type="bibr" coords="3,82.98,599.93,3.99,6.91">6</ref></s><s coords="3,90.85,601.97,199.41,9.46;3,72.00,615.52,218.27,9.46;3,72.00,629.07,218.27,9.46;3,72.00,642.62,96.95,9.46">Starting from the trained SciBERT model, we pretrain the Transformer parameters on the citation objective to learn paper representations that capture document relatedness.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4" coords="3,72.00,665.88,164.29,9.81">Selecting Negative Distractors</head><p><s coords="3,71.66,684.53,173.44,9.46;3,247.95,683.18,7.59,18.93;3,256.43,682.23,6.59,6.99;3,266.36,684.53,25.71,9.46;3,72.00,698.08,142.20,9.46">The choice of negative example papers P − is important when training the model.</s><s coords="3,217.60,698.08,72.67,9.46;3,72.00,711.63,220.08,9.46;3,72.00,725.18,220.18,9.46">We consider two sets of negative examples: the first set simply consists of randomly selected papers from the corpus.</s></p><p><s coords="3,307.28,66.67,218.27,9.46;3,307.28,80.22,218.27,9.46;3,307.28,93.76,218.27,9.46;3,307.28,107.31,79.64,9.46">Given a query paper, intuitively we would expect the model to be able to distinguish between cited papers, and uncited papers sampled randomly from the entire corpus.</s><s coords="3,394.50,107.31,131.05,9.46;3,307.28,120.86,218.27,9.46;3,307.28,134.41,131.15,9.46">This inductive bias has been also found to be effective in content-based citation recommendation applications .</s><s coords="3,441.14,134.41,4.89,9.46">But, random negatives may be easy for the model to distinguish from the positives.</s><s coords="3,446.03,134.41,64.57,9.46">To provide a more nuanced training signal, we augment the randomly drawn negatives with a more challenging second set of negative examples.</s><s coords="3,513.32,134.41,13.58,9.46">We denote as "hard negatives" the papers that are not cited by the query paper, but are cited by a paper cited by the query paper, i.e. if P 1 cite − − → P 2 and P 2 cite − − → P 3 but P 1 cite − − → P 3 , then P 3 is a candidate hard negative example for P 1 .</s><s coords="3,307.28,147.96,18.86,9.46">We expect the hard negatives to be somewhat related to the query paper, but typically less related than the cited papers.</s><s coords="3,326.14,147.96,9.43,9.46">As we show in our experiments ( §6), including hard negatives results in more accurate embeddings compared to using random negatives alone.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5" coords="3,307.28,365.47,68.57,9.81">Inference</head><p><s coords="3,306.88,384.03,206.23,9.46;3,515.75,382.68,11.16,18.93;3,307.28,397.58,218.27,9.46;3,307.28,411.13,207.06,9.46;3,517.06,409.77,7.59,18.93;3,306.92,424.68,56.56,9.46">At inference time, the model receives one paper, P, and it outputs the SPECTER's Transfomer pooled output activation as the paper representation for P (Equation <ref type="formula" coords="3,351.88,424.68,3.87,9.46">1</ref>).</s><s coords="3,366.86,424.68,158.43,9.46;3,307.28,438.23,218.27,9.46;3,307.28,451.78,218.26,9.46;3,307.28,465.32,146.72,9.46">We note that for inference, SPECTER requires only the title and abstract of the given input paper; the model does not need any citation information about the input paper.</s><s coords="3,457.40,465.32,68.15,9.46;3,307.55,478.87,218.39,9.46;3,307.28,492.42,218.27,9.46;3,307.28,505.97,220.06,9.46">This means that SPECTER can produce embeddings even for new papers that have yet to be cited, which is critical for applications that target recent scientific papers.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" coords="3,307.28,529.74,189.62,10.75">SCIDOCS Evaluation Framework</head><p><s coords="3,307.28,552.63,220.08,9.46;3,307.28,566.18,218.27,9.46;3,307.28,579.72,218.65,9.46;3,307.28,593.27,218.27,9.46;3,307.28,606.82,139.57,9.46">Previous evaluations of scientific document representations in the literature tend to focus on small datasets over a limited set of tasks, and extremely high (99%+) AUC scores are already possible on these data for English documents .</s><s coords="3,448.93,606.82,5.23,9.46">New, larger and more diverse benchmark datasets are necessary.</s><s coords="3,454.16,606.82,20.90,9.46">Here, we introduce a new comprehensive evaluation framework to measure the effectiveness of scientific paper embeddings, which we call SCIDOCS.</s><s coords="3,477.14,606.82,7.72,9.46">The framework consists of diverse tasks, ranging from citation prediction, to prediction of user activity, to document classification and paper recommendation.</s><s coords="3,486.95,606.82,9.80,9.46">Note that SPECTER will not be further fine-tuned on any of the tasks; we simply plug in the embeddings as features for each task.</s><s coords="3,496.75,606.82,3.27,9.46">Below, we describe each of the tasks in detail and the evaluation data associated with it.</s><s coords="3,502.10,606.82,19.48,9.46">In addition to our training data, we release all the datasets associated with the evaluation tasks.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" coords="4,72.00,119.91,137.57,9.81">Document Classification</head><p><s coords="4,71.61,140.21,218.66,9.46;4,71.61,153.76,220.57,9.46">An important test of a document-level embedding is whether it is predictive of the class of the document.</s><s coords="4,72.00,167.31,218.27,9.46;4,72.00,180.86,77.88,9.46">Here, we consider two classification tasks in the scientific domain:</s></p><p><s coords="4,72.00,205.53,218.26,9.88;4,72.00,219.50,220.08,9.46;4,72.00,233.05,214.18,9.46;4,286.18,231.00,3.99,6.91">MeSH Classification In this task, the goals is to classify scientific papers according to their Medical Subject Headings (MeSH) <ref type="bibr" coords="4,205.29,233.05,76.19,9.46" target="#b30">(Lipscomb, 2000)</ref>. <ref type="bibr" coords="4,286.18,231.00,3.99,6.91">7</ref></s><s coords="4,71.49,246.60,218.78,9.46;4,72.00,260.15,218.27,9.46;4,72.00,273.70,220.08,9.46;4,72.00,287.25,218.26,9.46;4,72.00,300.80,124.68,9.46">e construct a dataset consisting of 23K academic medical papers, where each paper is assigned one of 11 top-level disease classes such as cardiovascular diseases, diabetes, digestive diseases derived from the MeSH vocabulary.</s><s coords="4,201.22,300.80,89.05,9.46;4,72.00,314.35,218.26,9.46;4,71.64,327.89,218.63,9.46;4,72.00,341.44,218.27,9.46;4,72.00,354.99,58.61,9.46">The most populated category is Neoplasms (cancer) with 5.4K instances (23.3% of the total dataset) while the category with least number of samples is Hepatitis (1.7% of the total dataset).</s><s coords="4,134.00,354.99,156.27,9.46;4,72.00,368.54,218.27,9.46;4,72.00,382.09,83.31,9.46">We follow the approach of <ref type="bibr" coords="4,252.20,354.99,38.07,9.46;4,72.00,368.54,53.98,9.46" target="#b13">Feldman et al. (2019)</ref> in mapping the MeSH vocabulary to the disease classes.</s></p><p><s coords="4,72.00,406.76,220.08,9.88;4,72.00,420.74,220.08,9.46;4,72.00,434.29,218.27,9.46;4,72.00,447.84,157.47,9.46;4,229.48,445.79,3.99,6.91;4,233.96,447.84,2.78,9.46">Paper Topic Classification This task is predicting the topic associated with a paper using the predefined topic categories of the Microsoft Academic Graph (MAG) <ref type="bibr" coords="4,140.78,447.84,88.70,9.46" target="#b45">(Sinha et al., 2015)</ref> 8 .</s><s coords="4,244.00,447.84,48.08,9.46;4,71.73,461.38,218.54,9.46;4,72.00,474.93,41.45,9.46">MAG provides a database of papers, each tagged with a list of topics.</s><s coords="4,117.10,474.93,173.55,9.46;4,72.00,488.48,218.27,9.46;4,72.00,502.03,125.95,9.46">The topics are organized in a hierarchy of 5 levels, where level 1 is the most general and level 5 is the most specific.</s><s coords="4,205.02,502.03,86.61,9.46;4,71.61,515.58,218.66,9.46;4,72.00,529.13,218.27,9.46;4,72.00,542.68,149.96,9.46">For our evaluation, we derive a document classification dataset from the level 1 topics, where a paper is labeled by its corresponding level 1 MAG topic.</s><s coords="4,225.35,542.68,64.93,9.46;4,72.00,556.23,218.27,9.46;4,71.18,569.78,215.91,9.46">We construct a dataset of 25K papers, almost evenly split over the 19 different classes of level 1 categories in MAG.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" coords="4,72.00,595.92,113.74,9.81">Citation Prediction</head><p><s coords="4,71.61,616.22,220.48,9.46;4,72.00,629.77,112.33,9.46">As argued above, citations are a key signal of relatedness between papers.</s><s coords="4,187.70,629.77,104.38,9.46;4,72.00,643.32,218.26,9.46;4,72.00,656.87,143.57,9.46">We test how well different paper representations can reproduce this signal through citation prediction tasks.</s><s coords="4,218.95,656.87,71.32,9.46;4,72.00,670.23,219.63,9.64;4,72.00,683.78,124.99,9.64">In particular, we focus on two sub-tasks: predicting direct citations, and predicting co-citations.</s><s coords="4,204.90,683.97,85.37,9.46;4,72.00,697.52,218.01,9.46;4,72.00,711.07,160.01,9.46">We frame these as ranking tasks and evaluate performance using MAP and nDCG, standard ranking metrics.</s></p><p><s coords="4,307.28,66.24,218.27,9.88;4,307.28,80.22,218.64,9.46;4,307.28,93.76,195.54,9.46">Direct Citations In this task, the model is asked to predict which papers are cited by a given query paper from a given set of candidate papers.</s><s coords="4,508.25,93.76,17.30,9.46;4,307.28,107.31,220.07,9.46;4,307.28,120.86,220.08,9.46;4,307.28,134.41,218.27,9.46;4,307.28,147.96,219.00,9.46;4,307.28,161.51,65.10,9.46">The evaluation dataset includes approximately 30K total papers from a held-out pool of papers, consisting of 1K query papers and a candidate set of up to 5 cited papers and 25 (randomly selected) uncited papers.</s><s coords="4,375.75,161.51,149.80,9.46;4,307.28,175.06,139.14,9.46">The task is to rank the cited papers higher than the uncited papers.</s><s coords="4,451.63,175.06,75.72,9.46;4,307.28,188.61,218.27,9.46;4,307.28,202.16,218.65,9.46;4,307.28,215.71,218.27,9.46;4,307.28,229.26,50.59,9.46">For each embedding method, we require only comparing the L2 distance between the raw embeddings of the query and the candidates, without any additional trainable parameters.</s></p><p><s coords="4,307.28,249.01,218.27,9.88;4,307.28,262.80,219.63,9.64;4,307.28,276.53,218.27,9.46;4,307.28,290.08,63.00,9.46">Co-Citations This task is similar to the direct citations but instead of predicting a cited paper, the goal is to predict a highly co-cited paper with a given paper.</s><s coords="4,375.79,290.08,149.76,9.46;4,307.28,303.63,218.27,9.46;4,307.28,317.18,218.27,9.46;4,307.28,330.73,218.27,9.46;4,307.28,344.28,218.27,9.46;4,307.28,357.83,14.67,9.46">Intuitively, if papers A and B are cited frequently together by several papers, this shows that the papers are likely highly related and a good paper representation model should be able to identify these papers from a given candidate set.</s><s coords="4,325.33,357.83,200.22,9.46;4,307.28,371.19,201.71,9.64">The dataset consists of 30K total papers and is constructed similar to the direct citations task.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3" coords="4,307.28,392.60,85.93,9.81">User Activity</head><p><s coords="4,306.94,410.51,218.60,9.46;4,307.28,424.05,218.45,9.46;4,307.28,437.60,220.08,9.46;4,307.28,451.15,132.42,9.46">The embeddings for similar papers should be close to each other; we use user activity as a proxy for identifying similar papers and test the model's ability to recover this information.</s><s coords="4,443.08,451.15,84.28,9.46;4,307.28,464.70,218.27,9.46;4,307.28,478.25,220.08,9.46;4,307.28,491.80,218.45,9.46;4,307.28,505.35,57.67,9.46">Multiple users consuming the same items as one another is a classic relatedness signal and forms the foundation for recommender systems and other applications <ref type="bibr" coords="4,489.52,491.80,36.20,9.46;4,307.28,505.35,52.88,9.46" target="#b42">(Schafer et al., 2007)</ref>.</s><s coords="4,371.11,505.35,154.43,9.46;4,306.88,518.90,218.66,9.46;4,307.28,532.45,218.27,9.46;4,307.28,546.00,32.93,9.46">In our case, we would expect that when users look for academic papers, the papers they view in a single browsing session tend to be related.</s><s coords="4,343.61,546.00,183.30,9.46;4,307.28,559.55,218.45,9.46;4,307.28,573.10,220.08,9.46;4,307.28,586.64,120.65,9.46">Thus, accurate paper embeddings should, all else being equal, be relatively more similar for papers that are frequently viewed in the same session than for other papers.</s><s coords="4,434.59,586.64,91.22,9.46;4,307.28,600.19,218.27,9.46;4,307.28,613.74,220.07,9.46;4,307.28,627.29,91.59,9.46">To build benchmark datasets to test embeddings on user activity, we obtained logs of user sessions from a major academic search engine.</s><s coords="4,402.25,627.29,123.30,9.46;4,307.28,640.84,218.27,9.46;4,307.28,654.39,55.76,9.46">We define the following two tasks on which we build benchmark datasets to test embeddings:</s></p><p><s coords="4,307.28,674.14,220.08,9.88;4,307.28,688.12,110.46,9.46">Co-Views Our co-views dataset consists of approximately 30K papers.</s><s coords="4,421.48,688.12,104.07,9.46;4,306.46,701.67,220.90,9.46;4,307.00,715.22,218.54,9.46;4,307.28,728.77,220.08,9.46;4,307.28,742.32,220.18,9.46">To construct it, we take 1K random papers that are not in our train or development set and associate with each one up to 5 frequently co-viewed papers and 25 randomly selected papers (similar to the approach for citations).</s><s coords="4,306.94,755.86,218.61,9.46;5,72.00,66.67,218.27,9.46;5,72.00,80.22,220.18,9.46">Then, we require the embedding model to rank the co-viewed papers higher than the random papers by comparing the L2 distances of raw embeddings.</s><s coords="5,71.49,93.76,218.78,9.46;5,72.00,107.31,107.53,9.46">We evaluate performance using standard ranking metrics, nDCG and MAP.</s></p><p><s coords="5,72.00,128.57,218.57,9.88;5,72.00,142.54,218.27,9.46;5,72.00,156.09,220.07,9.46;5,72.00,169.64,15.96,9.46">Co-Reads If the user clicks to access the PDF of a paper from the paper description page, this is a potentially stronger sign of interest in the paper.</s><s coords="5,91.35,169.64,198.93,9.46;5,72.00,183.19,220.21,9.46;5,72.00,196.74,29.98,9.46">In such a case we assume the user will read at least parts of the paper and refer to this as a "read" action.</s><s coords="5,106.54,196.74,184.00,9.46;5,72.00,210.29,220.07,9.46;5,72.00,223.84,63.90,9.46">Accordingly, we define a "co-reads" task and dataset analogous to the co-views dataset described above.</s><s coords="5,139.35,223.84,151.30,9.46;5,72.00,237.39,52.71,9.46">This dataset is also approximately 30K papers.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4" coords="5,72.00,260.11,106.35,9.81">Recommendation</head><p><s coords="5,72.00,278.46,220.08,9.46;5,72.00,292.01,218.27,9.46;5,72.00,305.56,173.91,9.46">In the recommendation task, we evaluate the ability of paper embeddings to boost performance in a production recommendation system.</s><s coords="5,253.39,305.56,38.69,9.46;5,72.00,319.10,218.27,9.46;5,72.00,332.65,220.08,9.46;5,72.00,346.20,105.12,9.46">Our recommendation task aims to help users navigate the scientific literature by ranking a set of "similar papers" for a given paper.</s><s coords="5,181.73,346.20,108.72,9.46;5,72.00,359.75,218.27,9.46;5,72.00,373.30,218.65,9.46;5,72.00,386.85,64.04,9.46">We use a dataset of user clickthrough data for this task which consists of 22K clickthrough events from a public scholarly search engine.</s><s coords="5,140.76,386.85,151.32,9.46;5,72.00,400.40,219.63,9.46;5,72.00,413.95,75.78,9.46">We partitioned the examples temporally into train (20K examples), validation (1K), and test (1K) sets.</s><s coords="5,150.98,413.95,139.28,9.46;5,72.00,427.50,218.27,9.46;5,72.00,441.05,177.63,9.46">As is typical in clickthrough data on ranked lists, the clicks are biased toward the top of original ranking presented to the user.</s><s coords="5,253.04,441.05,39.05,9.46;5,72.00,454.60,218.27,9.46;5,72.00,468.15,220.17,9.46">To counteract this effect, we computed propensity scores using a swap experiment <ref type="bibr" coords="5,188.70,468.15,98.68,9.46" target="#b0">(Agarwal et al., 2019)</ref>.</s><s coords="5,71.66,481.69,218.61,9.46;5,72.00,495.24,218.27,9.46;5,72.00,508.79,220.18,9.46">The propensity scores give, for each position in the ranked list, the relative frequency that the position is over-represented in the data due to exposure bias.</s><s coords="5,71.49,522.34,218.78,9.46;5,72.00,535.89,218.27,9.46;5,72.00,549.44,173.11,9.46">We can then compute de-biased evaluation metrics by dividing the score for each test example by the propensity score for the clicked position.</s><s coords="5,248.38,549.44,41.89,9.46;5,72.00,562.99,220.08,9.46;5,72.00,576.19,218.27,9.81;5,72.00,590.09,165.65,9.46">We report propensity-adjusted versions of the standard ranking metrics Precision@1 (P @1) and Normalized Discounted Cumulative Gain (nDCG).</s></p><p><s coords="5,82.91,603.86,209.17,9.46;5,72.00,617.41,220.07,9.46;5,72.00,630.96,22.65,9.46;5,94.65,628.91,3.99,6.91;5,101.85,630.96,190.23,9.46;5,72.00,644.51,218.27,9.46;5,72.00,658.06,220.08,9.46;5,72.00,671.60,91.01,9.46">We test different embeddings on the recommendation task by including cosine embedding distance 9 as a feature within an existing recommendation system that includes several other informative features (title/author similarity, reference and citation overlap, etc.).</s><s coords="5,169.25,671.60,121.02,9.46;5,72.00,685.15,218.27,9.46;5,72.00,698.70,218.27,9.46;5,72.00,712.25,65.45,9.46">Thus, the recommendation experiments measure whether the embeddings can boost the performance of a strong baseline system on an end task.</s><s coords="5,140.85,712.25,149.42,9.46;5,72.00,725.80,218.27,9.46;5,307.28,66.67,218.27,9.46;5,307.28,80.22,176.34,9.46">For SPECTER, we also perform an online A/B test to measure whether its advantages on the offline dataset translate into improvements on the online recommendation task ( §5).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" coords="5,307.28,103.56,83.02,10.75">Experiments</head><p><s coords="5,307.28,125.71,218.27,9.88;5,307.28,139.69,187.03,9.46">Training Data To train our model, we use a subset of the Semantic Scholar corpus ) consisting of about 146K query papers (around 26.7M tokens) with their corresponding outgoing citations, and we use an additional 32K papers for validation.</s><s coords="5,494.31,139.69,31.42,9.46">For each query paper we construct up to 5 training triples comprised of a query, a positive, and a negative paper.</s><s coords="5,307.28,153.24,7.72,9.46">The positive papers are sampled from the direct citations of the query, while negative papers are chosen either randomly or from citations of citations (as discussed in §2.4).</s><s coords="5,317.56,153.24,9.80,9.46">We empirically found it helpful to use 2 hard negatives (citations of citations) and 3 easy negatives (randomly selected papers) for each query paper.</s><s coords="5,327.36,153.24,3.27,9.46">This process results in about 684K training triples and 145K validation triples.</s></p><p><s coords="5,307.28,350.59,218.26,9.88;5,307.28,364.57,111.02,9.46">Training and Implementation We implement our model in AllenNLP .</s><s coords="5,422.96,364.57,5.02,9.46">We initialize the model from SciBERT pretrained weights <ref type="bibr" coords="5,344.36,391.67,94.50,9.46" target="#b3">(Beltagy et al., 2019)</ref>   <ref type="bibr" coords="5,307.28,554.26,75.55,9.46" target="#b20">and Ruder, 2018)</ref> with number of train steps equal to training instances and cut fraction of 0.1).</s><s coords="5,427.97,364.57,35.14,9.46">We train the model on a single Titan V GPU (12G memory) for 2 epochs, with batch size of 4 (the maximum that fit in our GPU memory) and use gradient accumulation for an effective batch size of 32.</s><s coords="5,467.76,364.57,8.03,9.46">Each training epoch takes approximately 1-2 days to complete on the full dataset.</s><s coords="5,480.46,364.57,10.20,9.46">We release our code and data to facilitate reproducibility.</s><s coords="5,490.66,364.57,3.40,9.46"><ref type="bibr" coords="5,509.50,660.60,7.97,6.91">11</ref> Task-Specific Model Details For the classification tasks, we used a linear SVM where embedding vectors were the only features.</s><s coords="5,498.71,364.57,19.16,9.46">The C hyperparameter was tuned via a held-out validation set.</s></p><p><s coords="6,72.00,66.67,220.07,9.46;6,72.00,80.22,218.26,9.46;6,72.00,93.76,220.07,9.46;6,72.00,107.31,218.27,9.46;6,72.00,120.86,220.08,9.46;6,72.00,134.41,218.27,9.46;6,72.00,147.96,219.63,9.46;6,72.00,161.51,95.44,9.46">For the recommendation tasks, we use a feedforward ranking neural network that takes as input ten features designed to capture the similarity between each query and candidate paper, including the cosine similarity between the query and candidate embeddings and manually-designed features computed from the papers' citations, titles, authors, and publication dates.</s></p><p><s coords="6,72.00,182.11,220.08,9.88;6,72.00,196.09,219.63,9.46;6,72.00,209.64,220.08,9.46;6,72.00,223.19,199.87,9.46">Baseline Methods Our work falls into the intersection of textual representation, citation mining, and graph learning, and we evaluate against stateof-the-art baselines from each of these areas.</s><s coords="6,275.70,223.19,14.58,9.46;6,72.00,236.73,218.57,9.46;6,71.64,250.28,220.44,9.46;6,72.00,263.83,220.07,9.46;6,72.00,277.38,220.08,9.46;6,72.00,290.93,218.61,9.46;6,71.64,304.48,218.63,9.46;6,71.66,318.03,218.94,9.46;6,71.64,331.58,218.63,9.46;6,72.00,345.13,220.08,9.46;6,72.00,358.68,119.19,9.46">We compare with several strong textual models: SIF <ref type="bibr" coords="6,71.64,250.28,85.42,9.46" target="#b2">(Arora et al., 2017)</ref>, a method for learning document representations by removing the first principal component of aggregated word-level embeddings which we pretrain on scientific text; SciBERT <ref type="bibr" coords="6,71.64,304.48,94.18,9.46" target="#b3">(Beltagy et al., 2019)</ref> a state-of-the-art pretrained Transformer LM for scientific text; and Sent-BERT <ref type="bibr" coords="6,71.64,331.58,134.72,9.46" target="#b40">(Reimers and Gurevych, 2019)</ref>, a model that uses negative sampling to tune BERT for producing optimal sentence embeddings.</s><s coords="6,194.60,358.68,95.68,9.46;6,72.00,372.23,57.72,9.46">We also compare with Citeomatic , a closely related paper representation model for citation prediction which trains content-based representations with citation graph information via dynamically sampled triplets, and SGC <ref type="bibr" coords="6,195.64,426.42,80.90,9.46" target="#b50">(Wu et al., 2019a)</ref>, a state-of-the-art graph-convolutional approach.</s><s coords="6,129.72,372.23,80.07,9.46">For completeness, additional baselines are also included; due to space constraints we refer to Appendix A for detailed discussion of all baselines.</s><s coords="6,209.79,372.23,3.40,9.46">We tune hyperparameters of baselines to maximize performance on a separate validation set.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" coords="6,72.00,530.45,55.13,10.75">Results</head><p><s coords="6,71.66,552.59,218.61,9.46;6,72.00,566.14,160.43,9.46;6,235.21,564.79,16.90,18.93">Table <ref type="table" coords="6,100.05,552.59,5.56,9.46" target="#tab_1">1</ref> presents the main results corresponding to our evaluation tasks (described in §3).</s><s coords="6,255.64,566.14,35.99,9.46;6,71.61,579.69,218.66,9.46;6,72.00,593.24,218.27,9.46;6,72.00,606.79,218.27,9.46;6,72.00,620.34,179.73,9.46">Overall, we observe substantial improvements across all tasks with average performance of 80.0 across all metrics on all tasks which is a 3.1 point absolute improvement over the next-best baseline.</s><s coords="6,255.10,620.34,35.56,9.46;6,72.00,633.89,119.69,9.46">We now discuss the results in detail.</s></p><p><s coords="6,82.91,647.47,207.36,9.46;6,72.00,661.02,160.36,9.46">For document classification, we report macro F1, a standard classification metric.</s><s coords="6,237.83,661.02,52.44,9.46;6,72.00,674.57,218.45,9.46;6,72.00,688.12,218.65,9.46;6,72.00,701.67,64.64,9.46">We observe that the classifier performance when trained on our representations is better than when trained on any other baseline.</s><s coords="6,140.02,701.67,150.97,9.46;6,72.00,715.22,140.10,9.46">Particularly, on the MeSH (MAG) dataset, we obtain an 86.4 (82.0)</s><s coords="6,214.83,715.22,75.44,9.46;6,72.00,728.42,218.27,9.81;6,72.00,742.32,220.18,9.46">F1 score which is about a ∆= + 2.3 (+1.5) point absolute increase over the best baseline on each dataset respectively.</s><s coords="6,72.00,755.86,218.27,9.46;6,307.28,66.67,220.08,9.46;6,307.28,80.22,103.76,9.46">Our evaluation of the learned representations on predicting user activity is shown in the "User activity" columns of Table <ref type="table" coords="6,403.01,80.22,4.02,9.46" target="#tab_1">1</ref>.</s><s coords="6,414.70,80.22,110.59,9.46;6,307.28,93.76,220.08,9.46;6,307.28,107.31,218.27,9.46;6,307.28,120.86,220.17,9.46">SPECTER achieves a MAP score of 83.8 on the co-view task, and 84.5 on coread, improving over the best baseline (Citeomatic in this case) by 2.7 and 4.0 points, respectively.</s><s coords="6,306.76,134.41,218.78,9.46;6,305.83,147.96,219.72,9.46;6,307.00,161.51,218.54,9.46;6,307.28,175.06,218.27,9.46;6,307.28,188.61,22.57,9.46;6,329.84,186.56,7.97,6.91">We observe similar trends for the "citation" and "co-citation" tasks, with our model outperforming virtually all other baselines except for SGC, which has access to the citation graph at training and test time. <ref type="bibr" coords="6,329.84,186.56,7.97,6.91">12</ref></s><s coords="6,346.05,188.61,179.49,9.46;6,307.28,202.16,218.27,9.46;6,307.28,215.71,96.63,9.46">Note that methods like SGC cannot be used in real-world setting to embed new papers that are not cited yet.</s><s coords="6,409.69,215.71,117.66,9.46;6,307.28,229.26,218.26,9.46;6,307.28,242.81,218.27,9.46;6,306.88,256.35,68.51,9.46">On the other hand, on cocitation data our method is able to achieve the best results with nDCG of 94.8, improving over SGC with 2.3 points.</s><s coords="6,378.77,256.35,146.77,9.46;6,307.28,269.90,218.65,9.46;6,307.28,283.45,156.77,9.46">Citeomatic also performs well on the citation tasks, as expected given that its primary design goal was citation prediction.</s><s coords="6,467.44,283.45,59.47,9.46;6,307.28,297.00,218.27,9.46;6,307.28,310.55,220.08,9.46;6,307.28,324.10,150.73,9.46">Nevertheless, our method slightly outperforms Citeomatic on the direct citation task, while substantially outperforming it on co-citations (+2.0 nDCG).</s></p><p><s coords="6,318.19,343.93,207.36,9.46;6,307.28,357.47,218.27,9.46;6,307.28,371.02,148.22,9.46">Finally, for recommendation task, we observe that SPECTER outperforms all other models on this task as well, with nDCG of 53.9.</s><s coords="6,461.47,371.02,65.88,9.46;6,307.28,384.57,220.07,9.46;6,307.28,398.12,220.08,9.46;6,307.28,411.67,64.26,9.46">On the recommendations task, as opposed to previous experiments, the differences in method scores are generally smaller.</s><s coords="6,378.67,411.67,146.88,9.46;6,307.28,425.22,220.08,9.46;6,307.28,438.77,218.27,9.46;6,307.28,452.32,124.32,9.46;6,434.05,450.97,16.43,18.93;6,452.98,452.32,74.38,9.46;6,307.28,465.87,218.27,9.46;6,307.28,479.42,104.18,9.46">This is because for this task the embeddings are used along with several other informative features in the ranking model (described under task-specific models in §4), meaning that embedding variants have less opportunity for impact on overall performance.</s></p><p><s coords="6,318.19,499.24,207.36,9.46;6,306.88,512.79,220.47,9.46;6,307.28,526.34,111.54,9.46">We also performed an online study to evaluate whether SPECTER embeddings offer similar advantages in a live application.</s><s coords="6,422.18,526.34,103.36,9.46;6,306.88,539.89,220.47,9.46;6,307.28,553.44,220.08,9.46;6,307.28,566.99,218.27,9.46;6,307.28,580.54,82.08,9.46">We performed an online A/B test comparing our SPECTER-based recommender to an existing production recommender system for similar papers that ranks papers by a textual similarity measure.</s><s coords="6,392.74,580.54,132.80,9.46;6,307.28,594.09,218.27,9.46;6,307.28,607.64,218.27,9.46;6,307.28,621.19,49.29,9.46">In a dataset of 4,113 clicks, we found that SPECTER ranker improved clickthrough rate over the baseline by 46.5%, demonstrating its superiority.</s></p><p><s coords="6,318.19,641.01,209.17,9.46;6,307.28,654.56,218.27,9.46;6,307.55,668.11,219.81,9.46;6,307.28,681.66,220.10,9.46">We emphasize that our citation-based pretraining objective is critical for the performance of SPECTER; removing this and using a vanilla SciB-ERT results in decreased performance on all tasks.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" coords="7,72.00,280.81,61.12,10.75">Analysis</head><p><s coords="7,72.00,305.33,220.07,9.46;7,72.00,318.88,218.26,9.46;7,72.00,332.43,218.27,9.46;7,72.27,345.98,219.80,9.46;7,72.00,359.53,73.32,9.46">In this section, we analyze several design decisions in SPECTER, provide a visualization of its embedding space, and experimentally compare SPECTER's use of fixed embeddings against a finetuning approach.</s></p><p><s coords="7,72.00,383.45,218.66,9.88;7,72.00,397.43,220.08,9.46;7,72.00,410.97,162.93,9.46">Ablation Study We start by analyzing how adding or removing metadata fields from the input to SPECTER alters performance.</s><s coords="7,240.47,410.97,49.79,9.46;7,72.00,424.52,218.45,9.46;7,72.00,438.07,220.08,9.46;7,72.00,451.62,93.29,9.46">The results are shown in the top four rows of Table <ref type="table" coords="7,263.83,424.52,5.56,9.46" target="#tab_3">2</ref> (for brevity, here we only report the average of the metrics from each task).</s><s coords="7,171.28,451.62,119.00,9.46;7,72.00,465.17,218.65,9.46;7,72.00,478.72,220.08,9.46;7,72.00,492.27,43.57,9.46">We observe that removing the abstract from the textual input and relying only on the title results in a substantial decrease in performance.</s><s coords="7,118.95,492.27,171.32,9.46;7,72.00,505.82,220.07,9.46;7,72.00,519.37,31.82,9.46;7,103.82,517.32,7.97,6.91">More surprisingly, adding authors as an input (along with title and abstract) hurts performance. <ref type="bibr" coords="7,103.82,517.32,7.97,6.91">13</ref></s><s coords="7,116.36,519.37,174.10,9.46;7,72.00,532.92,218.27,9.46;7,72.00,546.47,218.27,9.46;7,72.00,560.02,49.41,9.46">One possible explanation is that author names are sparse in the corpus, making it difficult for the model to infer document-level relatedness from them.</s><s coords="7,124.83,560.02,167.24,9.46;7,72.00,573.57,218.27,9.46;7,72.00,587.11,128.18,9.46">As another possible reason of this behavior, tokenization using Wordpieces might be suboptimal for author names.</s><s coords="7,203.56,587.11,86.70,9.46;7,72.00,600.66,218.65,9.46;7,72.00,614.21,218.27,9.46;7,72.00,627.76,218.27,9.46;7,72.00,641.31,90.05,9.46">Many author names are out-of-vocabulary for SciBERT and thus, they might be split into sub-words and shared across names that are not semantically related, leading to noisy correlation.</s><s coords="7,166.69,641.31,123.58,9.46;7,71.73,654.86,177.14,9.46;7,248.86,652.81,7.97,6.91;7,261.39,654.86,28.88,9.46;7,72.00,668.41,218.27,9.46;7,71.61,681.96,218.66,9.46;7,81.66,704.54,5.98,5.18;7,88.14,706.40,202.13,7.77;7,72.00,716.37,218.27,7.77;7,72.00,726.33,18.18,7.77">Finally, we find that adding venues slightly decreases performance, 14 except on document classification (which makes sense, as we would expect venues to have high correlation <ref type="bibr" coords="7,81.66,704.54,5.98,5.18">13</ref> We experimented with both concatenating authors with the title and abstract and also considering them as an additional field.</s><s coords="7,92.96,726.33,76.44,7.77">Neither were helpful.</s></p><p><s coords="7,81.66,735.34,5.98,5.18;7,88.14,737.20,203.62,7.77;7,72.00,747.17,193.36,7.77">14 Venue information in our data came directly from publisher provided metadata and thus was not normalized.</s><s coords="7,268.14,747.17,22.13,7.77;7,72.00,757.13,149.41,7.77">Venue normalization could help improve results.</s><s coords="7,306.88,458.68,79.53,9.46">with paper topics).</s><s coords="7,389.72,458.68,135.83,9.46;7,307.28,472.23,220.08,9.46;7,307.28,485.78,220.08,9.46;7,307.28,499.33,218.27,9.46;7,307.28,512.87,218.27,9.46;7,307.28,526.42,198.64,9.46">The fact that SPECTER does not require inputs like authors or venues makes it applicable in situations where this metadata is not available, such as matching reviewers with anonymized submissions, or performing recommendations of anonymized preprints (e.g., on OpenReview).</s><s coords="7,318.19,541.16,207.36,9.46;7,307.28,554.71,220.08,9.46;7,307.28,568.26,74.75,9.46">One design decision in SPECTER is to use a set of hard negative distractors in the citation-based finetuning objective.</s><s coords="7,386.76,568.26,138.79,9.46;7,307.28,581.81,220.08,9.46;7,307.28,595.35,136.89,9.46">The fifth row of Table <ref type="table" coords="7,489.26,568.26,5.56,9.46" target="#tab_3">2</ref> shows that this is important-using only easy negatives reduces performance on all tasks.</s><s coords="7,447.58,595.35,77.96,9.46;7,307.28,608.90,218.27,9.46;7,307.28,622.45,220.08,9.46;7,307.28,636.00,126.25,9.46">While there could be other potential ways to include hard negatives in the model, our simple approach of including citations of citations is effective.</s><s coords="7,436.92,636.00,88.62,9.46;7,307.28,649.37,220.08,9.64;7,307.28,663.10,218.27,9.46;7,307.55,676.65,195.31,9.46">The sixth row of the table shows that using a strong general-domain language model (BERT-Large) instead of SciBERT in SPECTER reduces performance considerably.</s><s coords="7,506.27,676.65,19.28,9.46;7,307.28,690.20,220.07,9.46;7,307.28,703.75,154.32,9.46">This is reasonable because unlike BERT-Large, SciB-ERT is pretrained on scientific text.</s></p><p><s coords="7,307.28,728.34,218.45,9.88;7,307.28,742.32,218.27,9.46;7,306.92,755.86,218.62,9.46;8,72.00,239.30,129.55,9.46">Visualization Figure <ref type="figure" coords="7,412.66,728.77,5.56,9.46">2</ref> shows t-SNE (van der Maaten, 2014) projections of our embeddings (SPECTER) compared with the SciBERT baseline for a random set of papers.</s><s coords="8,211.27,239.30,79.00,9.46;8,72.27,252.85,218.00,9.46;8,72.00,266.40,220.07,9.46;8,72.00,279.95,218.27,9.46;8,72.00,293.50,39.77,9.46">When comparing SPECTER embeddings with SciBERT, we observe that our embeddings are better at encoding topical information, as the clusters seem to be more compact.</s><s coords="8,115.14,293.50,176.94,9.46;8,72.00,307.05,218.27,9.46;8,71.64,320.60,218.82,9.46;8,72.00,334.15,218.27,9.46;8,72.00,347.70,201.59,9.46">Further, we see some examples of crosstopic relatedness reflected in the embedding space (e.g., Engineering, Mathematics and Computer Science are close to each other, while Business and Economics are also close to each other).</s><s coords="8,278.78,347.70,11.49,9.46;8,72.00,361.25,218.27,9.46;8,72.00,374.79,220.07,9.46;8,72.00,388.34,220.17,9.46">To quantify the comparison of visualized embeddings in Figure <ref type="figure" coords="8,116.85,374.79,4.17,9.46">2</ref>, we use the DBScan clustering algorithm <ref type="bibr" coords="8,100.26,388.34,86.61,9.46" target="#b12">(Ester et al., 1996)</ref> on this 2D projection.</s><s coords="8,71.49,401.89,220.60,9.46;8,72.00,415.44,218.26,9.46;8,72.00,428.99,81.09,9.46">We use the completeness and homogeneity clustering quality measures introduced by <ref type="bibr" coords="8,226.14,415.44,64.12,9.46;8,72.00,428.99,76.64,9.46" target="#b41">Rosenberg and Hirschberg (2007)</ref>.</s><s coords="8,156.44,428.99,133.83,9.46;8,72.00,442.54,220.07,9.46;8,72.00,456.09,218.27,9.46;8,72.00,469.64,218.45,9.46;8,72.00,483.19,220.08,9.46;8,72.00,496.74,84.23,9.46">For the points corresponding to Figure <ref type="figure" coords="8,104.04,442.54,4.17,9.46">2</ref>, the homogeneity and completeness values for SPECTER are respectively 0.41 and 0.72 compared with SciBERT's 0.19 and 0.63, a clear improvement on separating topics using the projected embeddings.</s></p><p><s coords="8,72.00,525.10,218.27,9.81;8,71.49,539.08,220.59,9.46;8,72.00,552.63,218.27,9.46;8,72.00,566.18,218.27,9.46;8,71.66,579.72,220.41,9.46;8,72.00,593.27,144.55,9.46">Comparison with Task Specific Fine-Tuning While the fact that SPECTER does not require finetuning makes its paper embeddings less costly to use, often the best performance from pretrained Transformers is obtained when the models are finetuned directly on each end task.</s><s coords="8,222.29,593.27,67.99,9.46;8,71.61,606.82,218.66,9.46;8,72.00,620.37,220.08,9.46;8,72.00,633.92,124.51,9.46">We experiment with fine-tuning SciBERT on our tasks, and find this to be generally inferior to using our fixed representations from SPECTER.</s><s coords="8,199.89,633.92,92.20,9.46;8,72.00,647.47,220.08,9.46;8,72.00,661.02,80.78,9.46">Specifically, we finetune SciBERT directly on task-specific signals instead of citations.</s><s coords="8,160.23,661.02,130.04,9.46;8,72.00,674.57,220.07,9.46;8,71.73,688.12,218.93,9.46;8,72.00,701.67,218.27,9.46;8,72.00,715.22,144.34,9.46">To fine-tune on task-specific data (e.g., user activity), we used a dataset of coviews with 65K query papers, co-reads with 14K query papers, and co-citations (instead of direct citations) with 83K query papers.</s><s coords="8,219.72,715.22,70.55,9.46;8,72.00,728.77,218.27,9.46;8,72.00,742.32,218.27,9.46;8,72.00,755.86,54.56,9.46">As the end tasks are ranking tasks, for all datasets we construct up to 5 triplets and fine-tune the model using triplet ranking loss.</s><s coords="8,129.89,755.86,160.37,9.46;8,307.28,171.79,218.27,9.46;8,307.28,185.34,145.75,9.46">The positive papers are sampled from  the most co-viewed (co-read, or co-cited) papers corresponding to the query paper.</s><s coords="8,456.40,185.34,69.14,9.46;8,307.28,198.89,218.27,9.46;8,307.55,212.44,218.00,9.46;8,307.28,225.99,220.17,9.46">We also include both easy and hard distractors as when training SPECTER (for hard negatives we choose the least non-zero co-viewed (co-read, or co-cited) papers).</s><s coords="8,306.76,239.54,218.78,9.46;8,307.28,253.09,219.63,9.46;8,306.88,266.64,218.66,9.46;8,307.28,280.19,128.45,9.46">We also consider training jointly on all task-specific training data sources in a multitask training process, where the model samples training triplets from a distribution over the sources.</s><s coords="8,439.10,280.19,88.25,9.46;8,307.28,293.74,218.27,9.46;8,307.28,307.29,218.60,9.46;8,307.28,320.83,218.45,9.46;8,307.28,334.38,218.27,9.46;8,307.28,347.93,220.08,9.46;8,307.28,361.48,26.37,9.46;8,333.64,359.44,7.97,6.91">As illustrated in Table 3, without any additional final task-specific fine-tuning, SPECTER still outperforms a SciBERT model fine-tuned on the end tasks as well as their multitask combination, further demonstrating the effectiveness and versatility of SPECTER embeddings. <ref type="bibr" coords="8,333.64,359.44,7.97,6.91">15</ref></s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7" coords="8,307.28,383.83,89.09,10.75">Related Work</head><p><s coords="8,307.28,405.63,218.27,9.46;8,307.28,419.18,220.08,9.46;8,307.28,432.73,67.44,9.46">Recent representation learning methods in NLP rely on training large neural language models on unsupervised data <ref type="bibr" coords="8,464.60,432.73,62.31,9.46;8,307.28,446.28,24.63,9.46" target="#b38">Radford et al., 2018;</ref><ref type="bibr" coords="8,334.63,446.28,82.99,9.46" target="#b11">Devlin et al., 2019;</ref><ref type="bibr" coords="8,420.34,446.28,87.46,9.46" target="#b3">Beltagy et al., 2019;</ref><ref type="bibr" coords="8,510.53,446.28,15.02,9.46;8,307.28,459.83,51.58,9.46" target="#b33">Liu et al., 2019)</ref>.</s><s coords="8,377.46,432.73,4.32,9.46">While successful at many sentenceand token-level tasks, our focus is on using the models for document-level representation learning, which has remained relatively under-explored.</s></p><p><s coords="8,318.19,514.02,209.17,9.46;8,307.28,527.57,220.07,9.46;8,307.28,541.12,218.27,9.46;8,307.28,554.67,52.59,9.46">There have been other efforts in document representation learning such as extensions of word vectors to documents <ref type="bibr" coords="8,387.06,541.12,103.28,9.46" target="#b28">(Le and Mikolov, 2014;</ref><ref type="bibr" coords="8,493.06,541.12,32.49,9.46;8,307.28,554.67,52.59,9.46" target="#b14">Ganesh et al., 2016;</ref><ref type="bibr" coords="8,436.33,554.67,70.91,9.46" target="#b51">Wu et al., 2018;</ref><ref type="bibr" coords="8,510.05,554.67,11.54,9.46;8,307.28,568.22,66.48,9.46" target="#b16">Gysel et al., 2017)</ref>, convolution-based methods <ref type="bibr" coords="8,363.06,581.77,88.39,9.46" target="#b55">Zamani et al., 2018)</ref>, and variational autoencoders <ref type="bibr" coords="8,369.46,595.32,126.92,9.46" target="#b19">(Holmer and Marfurt, 2018;</ref>.</s><s coords="8,362.67,554.67,15.46,9.46">Relevant to document embedding, sentence embedding is a relatively well-studied area of research.</s><s coords="8,380.93,554.67,47.53,9.46">Successful approaches include seq2seq models <ref type="bibr" coords="8,344.34,649.52,88.76,9.46">(Kiros et al., 2015)</ref>, BiLSTM Siamese networks <ref type="bibr" coords="8,352.04,663.07,101.90,9.46" target="#b49">(Williams et al., 2018)</ref>, leveraging supervised data from other corpora <ref type="bibr" coords="8,456.29,676.61,70.61,9.46;8,307.28,690.16,23.95,9.46" target="#b10">(Conneau et al., 2017)</ref>, and using discourse relations <ref type="bibr" coords="8,477.32,690.16,49.58,9.46;8,307.28,703.71,23.95,9.46" target="#b35">(Nie et al., 2019)</ref>, and BERT-based methods <ref type="bibr" coords="8,464.16,703.71,61.39,9.46;8,307.28,717.26,74.24,9.46" target="#b40">(Reimers and Gurevych, 2019)</ref>.</s><s coords="8,428.46,554.67,5.07,9.46">Unlike our proposed method, the majority of these approaches do not consider any notion of inter-document relatedness when embedding documents.</s></p><p><s coords="9,82.91,108.18,207.36,9.46;9,71.61,121.72,165.09,9.46">Other relevant work combines textual features with network structure <ref type="bibr" coords="9,169.78,121.72,66.91,9.46" target="#b46">(Tu et al., 2017;</ref>.</s><s coords="9,238.97,121.72,49.39,9.46">These works typically do not leverage the recent pretrained contextual representations and with a few exceptions such as the recent work by , they cannot generalize to unseen documents like our SPECTER approach.</s><s coords="9,288.36,121.72,3.27,9.46">Context-based citation recommendation is another related application where models rely on citation contexts <ref type="bibr" coords="9,209.63,243.67,81.37,9.46" target="#b21">(Jeong et al., 2019)</ref> to make predictions.</s><s coords="9,72.00,135.27,20.28,9.46">These works are orthogonal to ours as the input to our model is just paper title and abstract.</s><s coords="9,92.28,135.27,5.07,9.46">Another related line of work is graphbased representation learning methods <ref type="bibr" coords="9,236.83,297.86,54.80,9.46;9,72.00,311.41,25.35,9.46" target="#b6">(Bruna et al., 2014;</ref><ref type="bibr" coords="9,101.79,311.41,112.87,9.46" target="#b24">Kipf and Welling, 2017;</ref><ref type="bibr" coords="9,219.09,311.41,72.54,9.46;9,72.00,324.96,37.42,9.46">Hamilton et al., 2017a,b;</ref><ref type="bibr" coords="9,112.15,324.96,80.33,9.46">Wu et al., 2019a,b)</ref>.</s><s coords="9,100.16,135.27,82.34,9.46">Here, we compare to a graph representation learning model, SGC (Simple Graph Convolution) <ref type="bibr" coords="9,178.97,352.06,76.98,9.46" target="#b50">(Wu et al., 2019a)</ref>, which is a state-of-the-art graph convolution approach for representation learning.</s><s coords="9,185.32,135.27,20.28,9.46">SPECTER uses pretrained language models in combination with graph-based citation signals, which enables it to outperform the graph-based approaches in our experiments.</s></p><p><s coords="9,83.18,434.22,207.08,9.46;9,72.00,447.77,105.68,9.46">SPECTER embeddings are based on only the title and abstract of the paper.</s><s coords="9,180.93,447.77,109.34,9.46;9,72.00,461.32,218.27,9.46;9,72.00,474.87,220.08,9.46;9,72.00,488.41,220.18,9.46">Adding the full text of the paper would provide a more complete picture of the paper's content and could improve accuracy <ref type="bibr" coords="9,271.68,474.87,15.30,9.46;9,72.00,488.41,69.58,9.46" target="#b9">(Cohen et al., 2010;</ref><ref type="bibr" coords="9,144.31,488.41,45.17,9.46" target="#b29">Lin, 2008;</ref><ref type="bibr" coords="9,192.21,488.41,95.30,9.46" target="#b43">Schuemie et al., 2004)</ref>.</s><s coords="9,72.00,501.96,218.27,9.46;9,72.00,515.51,98.93,9.46">However, the full text of many academic papers is not freely available.</s><s coords="9,175.02,515.51,115.26,9.46;9,72.00,529.06,219.63,9.46;9,71.61,542.61,218.66,9.46;9,72.00,556.16,218.27,9.46;9,72.00,569.71,51.93,9.46">Further, modern language models have strict memory limits on input size, which means new techniques would be required in order to leverage the entirety of the paper within the models.</s><s coords="9,129.35,569.71,161.10,9.46;9,72.00,583.26,205.37,9.46">Exploring how to use the full paper text within SPECTER is an item of future work.</s></p><p><s coords="9,82.91,597.67,209.17,9.46;9,72.00,611.22,218.65,9.46;9,72.00,624.77,218.27,9.46;9,72.00,638.32,58.86,9.46">Finally, one pain point in academic paper recommendation research has been a lack of publicly available datasets <ref type="bibr" coords="9,152.97,624.77,97.19,9.46" target="#b8">(Chen and Lee, 2018;</ref><ref type="bibr" coords="9,253.20,624.77,37.08,9.46;9,72.00,638.32,54.07,9.46" target="#b22">Kanakia et al., 2019)</ref>.</s><s coords="9,138.80,638.32,153.28,9.46;9,72.00,651.87,218.26,9.46;9,72.00,665.42,218.27,9.46;9,72.00,678.97,156.35,9.46">To address this challenge, we release SCIDOCS, our evaluation benchmark which includes an anonymized clickthrough dataset from an online recommendations system.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8" coords="9,72.00,704.57,171.93,10.75">Conclusions and Future Work</head><p><s coords="9,71.49,728.77,220.59,9.46;9,72.00,742.32,220.07,9.46;9,72.00,755.86,220.08,9.46;9,307.28,66.67,24.42,9.46">We present SPECTER, a model for learning representations of scientific papers, based on a Transformer language model that is pretrained on cita-tions.</s><s coords="9,335.61,66.67,190.12,9.46;9,307.28,80.22,220.08,9.46;9,307.28,93.76,179.63,9.46">We achieve substantial improvements over the strongest of a wide variety of baselines, demonstrating the effectiveness of our model.</s><s coords="9,494.46,93.76,32.89,9.46;9,307.28,107.31,218.27,9.46;9,307.28,120.86,218.27,9.46;9,307.28,134.41,218.45,9.46;9,307.28,147.96,89.66,9.46">We additionally introduce SCIDOCS, a new evaluation suite consisting of seven document-level tasks and release the corresponding datasets to foster further research in this area.</s></p><p><s coords="9,318.19,162.12,207.36,9.46;9,307.28,175.67,218.27,9.46;9,307.28,189.22,118.12,9.46">The landscape of Transformer language models is rapidly changing and newer and larger models are frequently introduced.</s><s coords="9,433.24,189.22,94.12,9.46;9,307.28,202.77,220.08,9.46;9,307.28,216.32,218.27,9.46;9,307.28,229.87,80.67,9.46">It would be interesting to initialize our model weights from more recent Transformer models to investigate if additional gains are possible.</s><s coords="9,391.33,229.87,134.21,9.46;9,307.28,243.42,218.27,9.46;9,307.28,256.97,218.27,9.46;9,307.28,270.51,36.63,9.46">Another item of future work is to develop better multitask approaches to leverage multiple signals of relatedness information during training.</s><s coords="9,347.29,270.51,178.44,9.46;9,307.28,284.06,218.27,9.46;9,307.28,297.61,218.27,9.46;9,306.92,311.16,218.63,9.46;9,307.28,324.71,170.19,9.46">We used citations to build triplets for our loss function, however there are other metrics that have good support from the bibliometrics literature <ref type="bibr" coords="9,306.92,311.16,116.48,9.46" target="#b26">(Klavans and Boyack, 2006</ref>) that warrant exploring as a way to create relatedness graphs.</s><s coords="9,482.90,324.71,42.65,9.46;9,307.28,338.26,220.08,9.46;9,307.28,351.81,218.45,9.46;9,307.28,365.36,107.38,9.46">Including other information such as outgoing citations as additional input to the model would be yet another area to explore in future.</s></p><p><s coords="12,72.00,65.52,172.39,10.75;12,72.00,86.38,218.27,9.88;12,71.61,100.35,211.14,9.46">A Appendix A -Baseline Details 1. Random Zero-mean 25-dimensional vectors were used as representations for each document.</s></p><p><s coords="12,72.00,117.17,218.27,9.88;12,72.00,131.15,218.27,9.46;12,72.00,144.70,220.17,9.46">2. Doc2Vec Doc2Vec is one of the earlier neural document/paragraph representation methods <ref type="bibr" coords="12,274.83,131.15,15.44,9.46;12,72.00,144.70,86.70,9.46" target="#b28">(Le and Mikolov, 2014)</ref>, and is a natural comparison.</s><s coords="12,71.49,158.25,218.78,9.46;12,72.00,171.80,218.27,9.46;12,72.00,185.34,218.27,9.46;12,72.00,198.89,95.33,9.46">We trained Doc2Vec on our training subset using Gensim <ref type="bibr" coords="12,108.53,171.80,115.17,9.46">(Řehůřek and Sojka, 2010)</ref>, and chose the hyperparameter grid using suggestions from <ref type="bibr" coords="12,272.97,185.34,17.30,9.46;12,72.00,198.89,90.69,9.46" target="#b27">Lau and Baldwin (2016)</ref>.</s><s coords="12,176.17,198.89,114.10,9.46;12,72.00,212.44,23.03,9.46">The hyperparameter grid used:</s></p><p><s coords="12,72.00,235.75,150.54,7.68;12,78.55,249.30,222.55,9.59;12,78.55,262.85,170.18,7.68;12,72.00,284.10,113.55,9.46">{'window': <ref type="bibr" coords="12,144.00,235.75,19.64,7.68">[5,</ref><ref type="bibr" coords="12,170.18,235.75,19.64,7.68">10,</ref><ref type="bibr" coords="12,196.36,235.75,19.64,7.68">15]</ref>, 'sample': [0, 10 ** -6, 10 ** -5], 'epochs': <ref type="bibr" coords="12,144.00,262.85,26.18,7.68">[50,</ref><ref type="bibr" coords="12,176.73,262.85,26.18,7.68">100,</ref><ref type="bibr" coords="12,209.45,262.85,19.64,7.68">200</ref>]}, for a total of 27 models.</s><s coords="12,193.08,284.10,97.20,9.46;12,71.61,297.65,220.02,9.46;12,72.00,312.22,74.78,7.68;12,216.85,312.22,74.78,7.68">The other parameters were set as follows: vector_size=300, min_count=3, alpha=0.025,</s><s coords="12,72.00,325.77,107.51,7.68">min_alpha=0.0001,</s><s coords="12,186.97,325.77,104.66,7.68;12,72.00,339.32,126.00,7.68">negative=5, dm=0, dbow=1, dbow_words=0.</s><s coords="12,72.00,355.12,218.26,9.88;12,71.61,369.09,194.77,9.46">3. Fasttext-Sum This simple baseline is a weighted sum of pretrained word vectors.</s><s coords="12,275.69,369.09,14.58,9.46;12,72.00,382.64,220.07,9.46;12,72.00,396.19,218.27,9.46;12,72.00,409.74,218.27,9.46;12,72.00,423.29,218.65,9.46;12,72.00,436.84,53.01,9.46">We trained our own 300 dimensional fasttext embeddings <ref type="bibr" coords="12,100.49,396.19,117.29,9.46" target="#b5">(Bojanowski et al., 2017)</ref> on a corpus of around 3.1B tokens from scientific papers which is similar in size to the SciBERT corpus <ref type="bibr" coords="12,252.33,423.29,38.32,9.46;12,72.00,436.84,48.40,9.46" target="#b3">(Beltagy et al., 2019)</ref>.</s><s coords="12,128.24,436.84,163.84,9.46;12,72.00,450.39,220.08,9.46;12,72.00,463.94,77.22,9.46">We found that these pretrained embeddings substantially outperform alternative off-theshelf embeddings.</s><s coords="12,152.40,463.94,137.87,9.46;12,72.00,477.49,218.27,9.46;12,71.64,491.04,198.05,9.46">We also use these embeddings in other baselines that require pretrained word vectors (i.e., SIF and SGC that are described below).</s><s coords="12,273.09,491.04,17.18,9.46;12,72.00,504.59,218.45,9.46;12,72.00,518.13,218.26,9.46;12,72.00,531.68,175.75,9.46">The summed bag of words representation has a number of weighting options, which are extensively tuned on a validation set for best performance.</s><s coords="12,72.00,548.50,218.26,9.88;12,72.00,562.48,218.27,9.46;12,71.61,576.03,218.65,9.46;12,72.00,589.58,220.07,9.46;12,72.00,603.13,218.27,9.46;12,72.00,616.68,218.27,9.46;12,72.00,630.22,220.08,9.46;12,72.00,643.77,32.42,9.46">4. SIF The SIF method of <ref type="bibr" coords="12,194.96,548.93,84.53,9.46" target="#b2">Arora et al. (2017)</ref> is a strong text representation baseline that takes a weighted sum of pretrained word vectors (we use fasttext embeddings described above), then computes the first principal component of the document embedding matrix and subtracts out each document embedding's projection to the first principal component.</s></p><p><s coords="12,71.49,660.67,218.78,9.81;12,72.00,674.57,218.65,9.46;12,72.00,688.12,67.19,9.46">We used a held-out validation set to choose a from the range [1.0e-5, 1.0e-3] spaced evenly on a log scale.</s><s coords="12,146.26,687.77,144.00,9.81;12,72.00,701.67,158.01,9.46">The word probability p(w) was estimated on the training set only.</s><s coords="12,238.29,701.67,53.79,9.46;12,72.00,715.22,218.27,9.46;12,72.00,728.77,220.07,9.46;12,72.00,742.32,220.07,9.46;12,72.00,755.86,20.10,9.46">When computing term-frequency values for SIF, we used scikit-learn's TfidfVectorizer with the same parameters as enumerated in the preceding section.</s><s coords="12,105.69,756.89,185.94,7.68;12,307.28,66.67,170.20,9.46">sublinear_tf, binary, use_idf, smooth_idf were all set to False.</s><s coords="12,481.96,66.67,43.90,9.46;12,307.28,80.22,218.27,9.46;12,307.28,93.76,96.68,9.46">Since SIF is a sum of pretrained fasttext vectors, the resulting dimensionality is 300.</s><s coords="12,383.70,112.06,4.24,9.46">provides contextualized representations of tokens in a document.</s><s coords="12,387.94,112.06,25.44,9.46">It can provide paragraph or document embeddings by averaging each token's representation for all 3 LSTM layers.</s><s coords="12,415.90,112.06,20.04,9.46">We used the 768-dimensional pretrained ELMo model in AllenNLP .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5." coords="12,307.28,111.63,73.90,9.88">ELMo ELMo</head><p><s coords="12,307.28,211.22,220.07,9.88;12,307.28,225.20,24.72,9.46">6. Citeomatic The most relevant baseline is Citeomatic , which is an academic paper representation model that is trained on the citation graph via sampled triplets.</s><s coords="12,334.82,225.20,4.90,9.46">Citeomatic representations are an L2 normalized weighted sum of title and abstract embeddings, which are trained on the citation graph with dynamic negative sampling.</s><s coords="12,339.71,225.20,53.89,9.46">Citeomatic embeddings are 75-dimensional.</s><s coords="12,396.42,225.20,24.45,9.46">7. SGC Since our algorithm is trained on data from the citation graph, we also compare to a state-ofthe-art graph representation learning model: SGC (Simple Graph Convolution) <ref type="bibr" coords="12,440.51,365.44,81.58,9.46" target="#b50">(Wu et al., 2019a)</ref>, which is a graph convolution network.</s><s coords="12,423.68,225.20,28.74,9.46">An alternative comparison would have been Graph-SAGE <ref type="bibr" coords="12,339.97,406.08,108.99,9.46" target="#b18">(Hamilton et al., 2017b)</ref>, but SGC (with no learning) outperformed an unsupervised variant of GraphSAGE on the Reddit dataset 16 , Note that SGC with no learning boils down to graph propagation on node features (in our case nodes are academic documents).</s><s coords="12,455.26,225.20,37.43,9.46">Following <ref type="bibr" coords="12,460.27,473.83,67.19,9.46;12,306.92,487.38,32.55,9.46" target="#b17">Hamilton et al. (2017a)</ref>, we used SIF features as node representations, and applied SGC with a range of parameter k, which is the number of times the normalized adjacency is multiplied by the SIF feature matrix.</s><s coords="12,495.51,225.20,10.50,9.46">Our range of k was 1 through 8 (inclusive), and was chosen with a validation set.</s><s coords="12,508.82,225.20,18.53,9.46">For the node features, we chose the SIF model with a = 0.0001, as this model was observed to be a high-performing one.</s><s coords="12,307.28,238.75,26.13,9.46">This baseline is also 300 dimensional.</s></p><p><s coords="12,307.28,613.64,218.27,9.88;12,307.55,627.62,219.36,9.46;12,306.88,641.17,219.00,9.46;12,306.92,654.72,102.85,9.46">8. SciBERT To isolate the advantage of SPECTER's citation-based fine-tuning objective, we add a controlled comparison with SciBERT <ref type="bibr" coords="12,306.92,654.72,98.06,9.46" target="#b3">(Beltagy et al., 2019)</ref>.</s><s coords="12,419.11,654.72,108.33,9.46;12,306.92,668.26,220.44,9.46;12,307.28,681.81,218.27,9.46;12,307.28,695.36,110.28,9.46;12,417.55,693.32,7.97,6.91">Following <ref type="bibr" coords="12,469.15,654.72,58.30,9.46;12,306.92,668.26,29.67,9.46" target="#b11">Devlin et al. (2019)</ref> we take the last layer hidden state corresponding to the [CLS] token as the aggregate document representation. 17</s><s coords="13,72.00,66.67,8.18,9.46">9.</s><s coords="13,85.16,66.24,205.10,9.88;13,72.00,80.22,218.27,9.46;13,72.00,93.76,175.64,9.46">Sentence BERT Sentence BERT <ref type="bibr" coords="13,232.41,66.67,57.85,9.46;13,72.00,80.22,70.87,9.46" target="#b40">(Reimers and Gurevych, 2019</ref>) is a general-domain pretrained model aimed at embedding sentences.</s><s coords="13,256.13,93.76,35.94,9.46;13,72.00,107.31,218.27,9.46;13,72.00,120.86,218.27,9.46;13,72.00,134.41,220.08,9.46;13,72.00,147.96,195.46,9.46">The authors fine-tuned BERT using a triplet loss, where positive sentences were from the same document section as the seed sentence, and distractor sentences came from other document sections.</s><s coords="13,272.97,147.96,17.30,9.46;13,72.00,161.51,218.26,9.46;13,72.00,175.06,220.07,9.46;13,72.00,188.61,220.07,9.46;13,72.00,202.16,218.27,9.46;13,72.00,215.71,176.35,9.46;13,248.34,213.66,7.97,6.91">The model is designed to encode sentences as opposed to paragraphs, so we embed the title and each sentence in the abstract separately, sum the embeddings, and L2 normalize the result to produce a final 768-dimensional paper embedding. <ref type="bibr" coords="13,248.34,213.66,7.97,6.91">18</ref></s><s coords="13,72.00,233.74,218.27,9.46;13,72.00,247.29,218.65,9.46;13,72.00,260.84,218.27,9.46;13,72.00,274.39,219.63,9.46;13,72.00,287.94,219.63,9.46;13,72.00,302.51,122.12,7.68">ring hyperparameter optimization we chose how to compute TF and IDF values weights by taking the following non-redundant combinations of scikit-learn's TfidfVectorizer <ref type="bibr" coords="13,215.14,274.39,76.49,9.46;13,72.00,287.94,25.96,9.46" target="#b36">(Pedregosa et al., 2011)</ref> parameters: sublinear_tf, binary, use_idf, smooth_idf.</s><s coords="13,203.65,301.49,86.62,9.46;13,72.00,315.03,136.17,9.46">There were a total of 9 parameter combinations.</s><s coords="13,217.48,315.03,72.79,9.46;13,71.61,328.58,163.60,9.46">The IDF values were estimated on the training set.</s><s coords="13,245.32,328.58,45.14,9.46;13,72.00,342.13,219.63,9.46;13,72.00,356.71,74.78,7.68">The other parameters were set as follows: min_df=3, max_df=0.75,</s><s coords="13,151.40,356.71,140.24,7.68;13,72.00,370.26,133.69,7.68;13,229.94,370.26,61.69,7.68;13,72.00,383.81,94.42,7.68">strip_accents='ascii', stop_words='english', norm=None, lowercase=True.</s><s coords="13,170.69,382.78,119.57,9.46;13,72.00,396.33,218.27,9.46;13,72.00,409.88,218.27,9.46;13,72.00,423.43,126.75,9.46">For training of fasttext, we used all default parameters with the exception of setting dimension to 300 and minCount was set to 25 due to the large corpus.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,350.08,157.88,132.65,8.64"><head>3Figure 1 :</head><label>1</label><figDesc><div><p><s coords="2,350.08,157.88,132.65,8.64">Figure 1: Overview of SPECTER.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,441.72,391.67,85.64,9.46;5,307.28,405.21,218.27,9.46;5,307.28,418.76,218.27,9.46;5,307.28,432.31,218.27,9.46;5,307.28,445.86,218.27,9.46;5,307.28,459.41,219.63,9.46;5,306.88,472.96,218.66,9.46;5,307.28,486.16,218.27,9.81;5,307.28,500.06,220.07,9.46;5,307.28,513.61,220.08,9.46;5,307.28,527.16,219.78,9.46;5,307.28,540.71,168.38,9.46;5,475.66,538.66,7.97,6.91;5,486.84,540.71,38.70,9.46"><head></head><label></label><figDesc><div><p><s coords="5,441.72,391.67,85.64,9.46;5,307.28,405.21,218.27,9.46;5,307.28,418.76,18.62,9.46">since it is the stateof-the-art pretrained language model on scientific text.</s><s coords="5,329.31,418.76,196.23,9.46;5,307.28,432.31,159.20,9.46">We continue training all model parameters on our training objective (Equation 2).</s><s coords="5,471.77,432.31,53.78,9.46;5,307.28,445.86,218.27,9.46;5,307.28,459.41,219.63,9.46;5,306.88,472.96,170.91,9.46">We perform minimal tuning of our model's hyperparameters based on the performance on the validation set, while baselines are extensively tuned.</s><s coords="5,483.66,472.96,41.88,9.46;5,307.28,486.16,218.27,9.81;5,307.28,500.06,50.25,9.46">Based on initial experiments, we use a margin m=1 for the triplet loss.</s><s coords="5,363.42,500.06,163.93,9.46;5,307.28,513.61,220.08,9.46;5,307.28,527.16,219.78,9.46;5,307.28,540.71,168.38,9.46;5,475.66,538.66,7.97,6.91;5,486.84,540.71,38.70,9.46">For training, we use the Adam optimizer (Kingma and Ba, 2014) following the suggested hyperparameters in Devlin et al. (2019) (LR: 2e-5, Slanted Triangular LR scheduler 10 (Howard</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,72.00,196.91,218.27,8.64;8,72.00,208.87,131.86,8.64"><head></head><label></label><figDesc><div><p><s coords="8,72.00,196.91,218.27,8.64;8,72.00,208.87,131.86,8.64">Figure 2: t-SNE visualization of paper embeddings and their corresponding MAG topics.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,307.28,67.87,54.23,7.77;8,429.42,67.87,96.51,7.77;8,307.50,83.31,34.35,7.77;8,430.17,82.96,95.75,8.12;8,307.28,93.67,218.65,7.77;8,307.28,103.63,218.65,7.77;8,307.28,113.60,218.65,7.77;8,307.28,123.56,218.65,7.77"><head></head><label></label><figDesc><div><p><s coords="8,307.28,93.67,218.65,7.77;8,307.28,103.63,218.65,7.77;8,307.28,113.60,218.65,7.77;8,307.28,123.56,218.65,7.77">SciBERT fine-tune on co-view 83.0 84.2 84.1 36.4 76.0 SciBERT fine-tune on co-read 82.3 85.4 86.7 36.3 77.1 SciBERT fine-tune on co-citation 82.9 84.3 85.2 36.6 76.4 SciBERT fine-tune on multitask 83.3 86.1 88.2 36.0 78.0</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,156.27,248.11,284.70,8.64"><head>Table 1 :</head><label>1</label><figDesc><div><p><s coords="7,191.49,248.11,249.48,8.64">Results on the SCIDOCS evaluation suite consisting of 7 tasks.</s></p></div></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,306.97,386.59,220.23,44.50"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note><p><s coords="7,344.84,386.59,180.70,8.64;7,307.28,398.54,219.65,8.64;7,307.28,410.50,219.92,8.64;7,307.28,422.45,65.95,8.64">Ablations: Numbers are averages of metrics for each evaluation task: CLS: classification, USR: User activity, CITE: Citation prediction, REC: Recommendation, Avg.</s><s coords="7,376.32,422.45,129.94,8.64">average over all tasks &amp; metrics.</s></p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,313.69,146.98,205.13,8.64"><head>Table 3 :</head><label>3</label><figDesc><div><p><s coords="8,348.91,146.98,169.91,8.64">Comparison with task-specific fine-tuning.</s></p></div></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">SPECTER: Scientific Paper Embeddings using Citationinformed TransformERs</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We also experimented with additional fields such as venues and authors but did not find any empirical advantage in using those (see §6). See §7 for a discussion of using the full text of the paper as input.5 It is also possible to encode title and abstracts individually and then concatenate or combine them to get the final embedding. However, in our experiments this resulted in sub-optimal performance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We also experimented with other distance functions (e..g, normalized cosine), but they underperformed the L2 loss.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://www.nlm.nih.gov/mesh/meshhome. html 8 https://academic.microsoft.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Embeddings are L2 normalized and in this case cosine distance is equivalent to L2 distance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">Learning rate linear warmup followed by linear decay. 11 https://github.com/allenai/specter</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">For SGC, we remove development and test set citations and co-citations during training. We also remove incoming citations from development and test set queries as these would not be available at test time in production.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">We also experimented with further task-specific finetuning of our SPECTER on the end tasks but we did not observe additional improvements.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">There were no other direct comparisons in<ref type="bibr" coords="12,491.26,716.37,35.85,7.77;12,306.98,726.33,27.89,7.77" target="#b50">Wu et al. (2019a)</ref> 17 We also tried the alternative of averaging all token representations, but this resulted in a slight performance decrease compared with the [CLS] pooled token.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18">We used the 'bert-base-wikipedia-sections-mean-tokens' model released by the authors: https://github.com/ UKPLab/sentence-transformers</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="9,307.28,390.09,98.83,10.75">Acknowledgements</head><p><s coords="9,306.76,413.66,218.78,9.46;9,307.28,427.21,218.45,9.46;9,307.28,440.76,218.27,9.46;9,307.28,454.31,218.27,9.46;9,306.92,467.86,220.44,9.46;9,307.28,481.41,24.42,9.46">We thank Kyle Lo, Daniel King and Oren Etzioni for helpful research discussions, Russel Reas for setting up the public API, Field Cady for help in initial data collection and the anonymous reviewers (especially Reviewer 1) for comments and suggestions.</s><s coords="9,338.25,481.41,187.59,9.46;9,307.28,494.96,218.27,9.46;9,307.28,508.51,218.27,9.46;9,306.76,522.06,171.55,9.46">This work was supported in part by NSF Convergence Accelerator award 1936940, ONR grant N00014-18-1-2193, and the University of Washington WRF/Cable Professorship.</s></p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="9,307.28,580.24,219.52,8.64;9,318.19,591.20,209.10,8.64;9,318.19,602.16,209.01,8.64;9,318.19,612.95,92.25,8.81" xml:id="b0">
	<analytic>
		<title level="a" type="main">Estimating position bias without intrusive interventions</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Anant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivan</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanhui</forename><surname>Zaitsev</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thorsten</forename><surname>Najork</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="raw_reference">Anant K. Agarwal, Ivan Zaitsev, Xuanhui Wang, Cheng Yen Li, Marc Najork, and Thorsten Joachims. 2019. Estimating position bias without intrusive in- terventions. In WSDM.</note>
</biblStruct>

<biblStruct coords="9,307.28,635.48,219.92,8.64;9,318.19,646.44,207.36,8.64;9,318.19,657.40,208.61,8.64;9,318.19,668.36,209.02,8.64;9,318.19,679.32,209.02,8.64;9,318.19,690.28,207.36,8.64;9,318.19,701.24,208.61,8.64;9,318.19,712.20,209.02,8.64;9,318.19,723.15,207.36,8.64;9,318.19,733.94,137.31,8.81" xml:id="b1">
	<analytic>
		<title level="a" type="main">Construction of the literature graph in semantic scholar</title>
		<author>
			<persName coords=""><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miles</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Dunkelberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmed</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vu</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Kohlmeier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tyler</forename><forename type="middle">C</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hsu-Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joanna</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sam</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Skjonsberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Madeleine</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oren</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="raw_reference">Waleed Ammar, Dirk Groeneveld, Chandra Bha- gavatula, Iz Beltagy, Miles Crawford, Doug Downey, Jason Dunkelberger, Ahmed Elgohary, Sergey Feldman, Vu Ha, Rodney Kinney, Sebas- tian Kohlmeier, Kyle Lo, Tyler C. Murray, Hsu- Han Ooi, Matthew E. Peters, Joanna Power, Sam Skjonsberg, Lucy Lu Wang, Christopher Wilhelm, Zheng Yuan, Madeleine van Zuylen, and Oren Et- zioni. 2018. Construction of the literature graph in semantic scholar. In NAACL-HLT.</note>
</biblStruct>

<biblStruct coords="9,307.28,756.48,220.01,8.64;10,82.55,67.28,209.37,8.64;10,82.91,78.07,76.92,8.81" xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName coords=""><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="raw_reference">Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A simple but tough-to-beat baseline for sentence em- beddings. In ICLR.</note>
</biblStruct>

<biblStruct coords="10,72.00,98.38,219.92,8.64;10,82.91,109.34,207.36,8.64;10,82.60,120.13,69.43,8.81" xml:id="b3">
	<analytic>
		<title level="a" type="main">SciB-ERT: A Pretrained Language Model for Scientific Text</title>
		<author>
			<persName coords=""><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="raw_reference">Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB- ERT: A Pretrained Language Model for Scientific Text. In EMNLP.</note>
</biblStruct>

<biblStruct coords="10,72.00,140.44,219.52,8.64;10,82.91,151.39,207.36,8.64;10,82.91,162.18,141.47,8.81" xml:id="b4">
	<analytic>
		<title level="a" type="main">Content-Based Citation Recommendation</title>
		<author>
			<persName coords=""><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="raw_reference">Chandra Bhagavatula, Sergey Feldman, Russell Power, and Waleed Ammar. 2018. Content-Based Citation Recommendation. In NAACL-HLT.</note>
</biblStruct>

<biblStruct coords="10,72.00,182.49,218.27,8.64;10,82.60,193.45,207.67,8.64;10,82.91,204.24,115.33,8.81" xml:id="b5">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00051</idno>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="raw_reference">Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. TACL.</note>
</biblStruct>

<biblStruct coords="10,72.00,224.55,218.27,8.64;10,82.33,235.51,208.29,8.64;10,82.91,246.30,150.96,8.81" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Spectral networks and locally connected networks on graphs. ICLR</title>
				<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="raw_reference">Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. 2014. Spectral networks and locally connected networks on graphs. ICLR.</note>
</biblStruct>

<biblStruct coords="10,72.00,266.60,219.92,8.64;10,82.91,277.56,207.36,8.64;10,82.44,288.52,209.48,8.64;10,82.91,299.48,209.01,8.64;10,82.91,310.27,150.51,8.81" xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving textual network embedding with global attention via optimal transport</title>
		<author>
			<persName coords=""><forename type="first">Liqun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pengyu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="raw_reference">Liqun Chen, Guoyin Wang, Chenyang Tao, Ding- han Shen, Pengyu Cheng, Xinyuan Zhang, Wenlin Wang, Yizhe Zhang, and Lawrence Carin. 2019. Im- proving textual network embedding with global at- tention via optimal transport. In ACL.</note>
</biblStruct>

<biblStruct coords="10,72.00,330.58,219.92,8.64;10,82.91,341.54,209.11,8.64;10,82.91,352.33,209.01,8.81;10,82.91,363.29,69.08,8.58" xml:id="b8">
	<analytic>
		<title level="a" type="main">Research Paper Recommender Systems on Big Scholarly Data</title>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Tsung Teng Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Management and Acquisition for Intelligent Systems</title>
				<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="raw_reference">Tsung Teng Chen and Maria Lee. 2018. Research Pa- per Recommender Systems on Big Scholarly Data. In Knowledge Management and Acquisition for In- telligent Systems.</note>
</biblStruct>

<biblStruct coords="10,72.00,383.59,219.92,8.64;10,82.91,394.55,209.10,8.64;10,82.91,405.51,207.36,8.64;10,82.66,416.47,209.35,8.64;10,82.60,427.26,137.82,8.81" xml:id="b9">
	<analytic>
		<title level="a" type="main">The structural and content aspects of abstracts versus bodies of full text journal articles are different</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Helen</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karin</forename><forename type="middle">M</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christophe</forename><surname>Roeder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lawrence</forename><surname>Hunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="492" to="492" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="raw_reference">K. Bretonnel Cohen, Helen L. Johnson, Karin M. Ver- spoor, Christophe Roeder, and Lawrence Hunter. 2010. The structural and content aspects of abstracts versus bodies of full text journal articles are different. BMC Bioinformatics, 11:492-492.</note>
</biblStruct>

<biblStruct coords="10,72.00,447.57,218.27,8.64;10,82.91,458.53,207.36,8.64;10,82.91,469.49,207.36,8.64;10,82.91,480.28,206.92,8.81" xml:id="b10">
	<analytic>
		<title level="a" type="main">Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</title>
		<author>
			<persName coords=""><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1070</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="raw_reference">Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. 2017. Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. In EMNLP.</note>
</biblStruct>

<biblStruct coords="10,72.00,500.58,218.27,8.64;10,82.91,511.54,207.36,8.64;10,82.91,522.50,209.02,8.64;10,82.91,533.29,104.95,8.81" xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In NAACL-HLT.</note>
</biblStruct>

<biblStruct coords="10,72.00,553.60,218.27,8.64;10,82.55,564.56,209.37,8.64;10,82.91,575.52,207.36,8.64;10,82.91,586.31,63.63,8.81" xml:id="b12">
	<analytic>
		<title level="a" type="main">A Density-based Algorithm for Discovering Clusters in Large Spatial Databases with Noise</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jörg</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note type="raw_reference">Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. 1996. A Density-based Algorithm for Dis- covering Clusters in Large Spatial Databases with Noise. In KDD.</note>
</biblStruct>

<biblStruct coords="10,72.00,606.61,219.92,8.64;10,82.91,617.57,209.10,8.64;10,82.91,628.53,207.36,8.64;10,82.44,639.32,164.65,8.81" xml:id="b13">
	<analytic>
		<title level="a" type="main">Quantifying Sex Bias in Clinical Studies at Scale With Automated Data Extraction</title>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elly</forename><surname>Trepman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Madeleine</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamanetworkopen.2019.6700</idno>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="raw_reference">Sergey Feldman, Waleed Ammar, Kyle Lo, Elly Trep- man, Madeleine van Zuylen, and Oren Etzioni. 2019. Quantifying Sex Bias in Clinical Studies at Scale With Automated Data Extraction. JAMA.</note>
</biblStruct>

<biblStruct coords="10,72.00,659.63,220.01,8.64;10,82.91,670.59,207.53,8.64;10,82.91,681.38,177.61,8.81" xml:id="b14">
	<analytic>
		<title level="a" type="main">Doc2sent2vec: A novel two-phase approach for learning document representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vijay</forename><forename type="middle">K</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="raw_reference">J Ganesh, Manish Gupta, and Vijay K. Varma. 2016. Doc2sent2vec: A novel two-phase approach for learning document representation. In SIGIR.</note>
</biblStruct>

<biblStruct coords="10,72.00,701.69,218.27,8.64;10,82.60,712.64,209.32,8.64;10,82.91,723.60,209.10,8.64;10,82.55,734.56,209.37,8.64;10,82.91,745.35,207.36,8.81;10,82.58,756.31,160.04,8.58" xml:id="b15">
	<analytic>
		<title level="a" type="main">AllenNLP: A Deep Semantic Natural Language Processing Platform</title>
		<author>
			<persName coords=""><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joel</forename><surname>Grus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-2501</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop for NLP Open Source Software</title>
				<meeting>Workshop for NLP Open Source Software<address><addrLine>NLP-OSS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="raw_reference">Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe- ters, Michael Schmitz, and Luke Zettlemoyer. 2018. AllenNLP: A Deep Semantic Natural Language Pro- cessing Platform. In Proceedings of Workshop for NLP Open Source Software (NLP-OSS).</note>
</biblStruct>

<biblStruct coords="10,307.28,67.28,219.92,8.64;10,318.19,78.24,209.01,8.64;10,318.19,89.03,209.10,8.81;10,317.94,99.99,18.54,8.58" xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural Vector Spaces for Unsupervised Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Christophe</forename><surname>Van Gysel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="raw_reference">Christophe Van Gysel, Maarten de Rijke, and Evange- los Kanoulas. 2017. Neural Vector Spaces for Un- supervised Information Retrieval. ACM Trans. Inf. Syst.</note>
</biblStruct>

<biblStruct coords="10,307.28,121.21,220.01,8.64;10,318.19,132.17,209.10,8.64;10,318.19,142.96,34.31,8.81" xml:id="b17">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName coords=""><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="raw_reference">Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017a. Inductive Representation Learning on Large Graphs. In NIPS.</note>
</biblStruct>

<biblStruct coords="10,307.28,164.17,220.01,8.64;10,318.19,175.13,207.36,8.64;10,318.19,185.92,66.95,8.81" xml:id="b18">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="raw_reference">William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017b. Inductive representation learning on large graphs. In NIPS.</note>
</biblStruct>

<biblStruct coords="10,307.28,207.14,218.27,8.64;10,318.19,218.10,209.01,8.64;10,318.19,228.89,147.75,8.81" xml:id="b19">
	<analytic>
		<title level="a" type="main">Explaining away syntactic structure in semantic document representations</title>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Holmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Marfurt</surname></persName>
		</author>
		<idno>abs/1806.01620</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="raw_reference">Erik Holmer and Andreas Marfurt. 2018. Explaining away syntactic structure in semantic document rep- resentations. ArXiv, abs/1806.01620.</note>
</biblStruct>

<biblStruct coords="10,307.28,250.11,218.27,8.64;10,318.19,261.06,209.10,8.64;10,318.19,271.85,31.25,8.81" xml:id="b20">
	<analytic>
		<title level="a" type="main">Universal Language Model Fine-tuning for Text Classification</title>
		<author>
			<persName coords=""><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1031</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Fine-tuning for Text Classification. In ACL.</note>
</biblStruct>

<biblStruct coords="10,307.28,293.07,219.92,8.64;10,318.19,304.03,207.72,8.64;10,318.19,314.99,207.36,8.64;10,318.19,325.78,208.60,8.81;10,318.19,336.91,65.86,8.64" xml:id="b21">
	<analytic>
		<title level="a" type="main">A context-aware citation recommendation model with bert and graph convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">Chanwoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sion</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hyuna</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucy</forename><surname>Eunjeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sungchul</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Choi</surname></persName>
		</author>
		<idno>abs/1903.06464</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="raw_reference">Chanwoo Jeong, Sion Jang, Hyuna Shin, Eun- jeong Lucy Park, and Sungchul Choi. 2019. A context-aware citation recommendation model with bert and graph convolutional networks. ArXiv, abs/1903.06464.</note>
</biblStruct>

<biblStruct coords="10,307.28,357.96,218.27,8.64;10,318.19,368.91,207.36,8.64;10,318.19,379.87,209.02,8.64;10,318.19,390.66,68.60,8.81" xml:id="b22">
	<analytic>
		<title level="a" type="main">A Scalable Hybrid Research Paper Recommender System for Microsoft Academic</title>
		<author>
			<persName coords=""><forename type="first">Anshul</forename><surname>Kanakia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Darrin</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="raw_reference">Anshul Kanakia, Zhihong Shen, Darrin Eide, and Kuansan Wang. 2019. A Scalable Hybrid Research Paper Recommender System for Microsoft Aca- demic. In WWW.</note>
</biblStruct>

<biblStruct coords="10,307.28,411.88,219.66,8.64;10,317.83,422.67,208.96,8.81;10,318.19,433.80,60.88,8.64" xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="raw_reference">Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. ArXiv, abs/1412.6980.</note>
</biblStruct>

<biblStruct coords="10,307.28,454.85,219.92,8.64;10,318.19,465.81,207.36,8.64;10,318.19,476.60,66.58,8.81" xml:id="b24">
	<monogr>
		<title level="m" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
	<note type="raw_reference">Thomas N Kipf and Max Welling. 2017. Semi- supervised classification with graph convolutional networks. ICLR.</note>
</biblStruct>

<biblStruct coords="10,307.28,497.81,219.52,8.64;10,318.19,508.77,209.02,8.64;10,318.19,519.73,209.10,8.64;10,318.19,530.52,34.31,8.81" xml:id="b25">
	<analytic>
		<title level="a" type="main">Raquel Urtasun, and Sanja Fidler. 2015. Skip-thought vectors</title>
		<author>
			<persName coords=""><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint/>
	</monogr>
	<note type="raw_reference">Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urta- sun, and Sanja Fidler. 2015. Skip-thought vectors. In NIPS.</note>
</biblStruct>

<biblStruct coords="10,307.28,551.74,219.92,8.64;10,318.19,562.70,207.36,8.64;10,318.19,573.49,207.36,8.81;10,317.94,584.45,150.74,8.81" xml:id="b26">
	<analytic>
		<title level="a" type="main">Identifying a better measure of relatedness for mapping science</title>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Klavans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Boyack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="251" to="263" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="raw_reference">Richard Klavans and Kevin W. Boyack. 2006. Iden- tifying a better measure of relatedness for mapping science. Journal of the Association for Information Science and Technology, 57:251-263.</note>
</biblStruct>

<biblStruct coords="10,307.28,605.66,190.21,8.64;10,513.37,605.66,12.17,8.64;10,318.19,616.62,209.01,8.64;10,318.19,627.58,207.36,8.64;10,317.88,638.37,68.37,8.58" xml:id="b27">
	<analytic>
		<title level="a" type="main">An empirical evaluation of doc2vec with practical insights into document embedding generation</title>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Jey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothy</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rep4NLP@ACL</title>
				<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jey Han Lau and Timothy Baldwin. 2016. An empirical evaluation of doc2vec with practical in- sights into document embedding generation. In Rep4NLP@ACL.</note>
</biblStruct>

<biblStruct coords="10,307.28,659.59,219.92,8.64;10,318.19,670.38,200.87,8.81" xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributed Representations of Sentences and Documents</title>
		<author>
			<persName coords=""><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="raw_reference">Quoc Le and Tomas Mikolov. 2014. Distributed Repre- sentations of Sentences and Documents. In ICML.</note>
</biblStruct>

<biblStruct coords="10,307.28,691.60,219.92,8.64;10,318.19,702.39,208.60,8.81;10,317.44,713.51,40.13,8.64" xml:id="b29">
	<analytic>
		<title level="a" type="main">Is searching full text more effective than searching abstracts?</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="46" to="46" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jimmy J. Lin. 2008. Is searching full text more effec- tive than searching abstracts? BMC Bioinformatics, 10:46-46.</note>
</biblStruct>

<biblStruct coords="10,307.28,734.56,218.27,8.64;10,317.86,745.35,209.34,8.81;10,318.19,756.31,17.99,8.58" xml:id="b30">
	<monogr>
		<title level="m" type="main">Bulletin of the Medical Library Association</title>
		<author>
			<persName coords=""><forename type="first">Carolyn</forename><forename type="middle">E</forename><surname>Lipscomb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Medical Subject Headings (MeSH)</note>
	<note type="raw_reference">Carolyn E Lipscomb. 2000. Medical Subject Headings (MeSH). Bulletin of the Medical Library Associa- tion.</note>
</biblStruct>

<biblStruct coords="11,72.00,67.28,220.01,8.64;11,82.91,78.24,207.36,8.64;11,82.91,89.03,135.03,8.81" xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised Document Embedding with CNNs</title>
		<author>
			<persName coords=""><forename type="first">Chundi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shunan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maksims</forename><surname>Volkovs</surname></persName>
		</author>
		<idno>abs/1711.04168v3</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="raw_reference">Chundi Liu, Shunan Zhao, and Maksims Volkovs. 2018. Unsupervised Document Embedding with CNNs. ArXiv, abs/1711.04168v3.</note>
</biblStruct>

<biblStruct coords="11,72.00,108.42,220.01,8.64;11,82.91,119.38,207.53,8.64;11,82.91,130.33,209.10,8.64;11,82.74,141.12,30.17,8.58" xml:id="b32">
	<monogr>
		<title level="m" type="main">A Model of Extended Paragraph Vector for Document Categorization and Trend Analysis</title>
		<author>
			<persName coords=""><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">King</forename><forename type="middle">Keung</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Helen</forename><forename type="middle">M</forename><surname>Meng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>IJCNN</note>
	<note type="raw_reference">Pengfei Liu, King Keung Wu, and Helen M. Meng. 2017. A Model of Extended Paragraph Vector for Document Categorization and Trend Analysis. IJCNN.</note>
</biblStruct>

<biblStruct coords="11,72.00,160.51,219.92,8.64;11,82.91,171.47,208.61,8.64;11,82.91,182.43,209.10,8.64;11,82.91,193.39,209.02,8.64;11,82.91,204.18,154.67,8.81" xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mandar</forename><forename type="middle">S</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
	</analytic>
	<monogr>
		<title level="j">RoBERTa: A Robustly Optimized BERT Pretraining Approach. ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="raw_reference">Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke S. Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretrain- ing Approach. ArXiv, abs/1907.11692.</note>
</biblStruct>

<biblStruct coords="11,72.00,223.56,218.27,8.64;11,82.91,234.35,207.36,8.81;11,82.63,245.31,78.07,8.58" xml:id="b34">
	<analytic>
		<title level="a" type="main">Accelerating t-SNE Using Tree-based Algorithms</title>
		<author>
			<persName coords=""><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="raw_reference">Laurens van der Maaten. 2014. Accelerating t-SNE Using Tree-based Algorithms. Journal of Machine Learning Research.</note>
</biblStruct>

<biblStruct coords="11,72.00,264.70,220.01,8.64;11,82.91,275.66,207.36,8.64;11,82.91,286.45,151.34,8.81" xml:id="b35">
	<analytic>
		<title level="a" type="main">DisSent: Learning Sentence Representations from Explicit Discourse Relations</title>
		<author>
			<persName coords=""><forename type="first">Allen</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erin</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1442</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="raw_reference">Allen Nie, Erin Bennett, and Noah Goodman. 2019. DisSent: Learning Sentence Representations from Explicit Discourse Relations. In ACL.</note>
</biblStruct>

<biblStruct coords="11,72.00,305.83,219.52,8.64;11,82.91,316.79,208.61,8.64;11,82.91,327.75,208.61,8.64;11,82.91,338.71,209.01,8.64;11,82.91,349.67,207.36,8.64;11,82.91,360.46,208.60,8.81;11,82.16,371.59,60.05,8.64" xml:id="b36">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="raw_reference">F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch- esnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825-2830.</note>
</biblStruct>

<biblStruct coords="11,72.00,390.80,218.27,8.64;11,82.91,401.76,207.36,8.64;11,82.91,412.72,209.02,8.64;11,82.91,423.68,50.08,8.64" xml:id="b37">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Deep Contextualized Word Representations</title>
				<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="raw_reference">Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word Rep- resentations.</note>
</biblStruct>

<biblStruct coords="11,72.00,442.90,218.27,8.64;11,82.91,453.86,209.02,8.64;11,82.91,464.65,170.03,8.81" xml:id="b38">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note>Tim Salimans, and Ilya Sutskever</note>
	<note type="raw_reference">Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing by generative pre-training. arXiv.</note>
</biblStruct>

<biblStruct coords="11,72.00,485.05,219.92,8.64;11,82.55,496.01,207.72,8.64;11,82.63,506.80,26.85,8.58" xml:id="b39">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName coords=""><forename type="first">Petr</forename><surname>Radimřehůřek</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sojka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
				<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="raw_reference">RadimŘehůřek and Petr Sojka. 2010. Software Frame- work for Topic Modelling with Large Corpora. In LREC.</note>
</biblStruct>

<biblStruct coords="11,72.00,526.19,219.92,8.64;11,82.91,537.15,209.01,8.64;11,82.91,547.94,90.65,8.81" xml:id="b40">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
		<author>
			<persName coords=""><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="raw_reference">Nils Reimers and Iryna Gurevych. 2019. Sentence- BERT: Sentence Embeddings using Siamese BERT- Networks. In EMNLP.</note>
</biblStruct>

<biblStruct coords="11,72.00,567.32,219.92,8.64;11,82.91,578.28,207.36,8.64;11,82.91,589.07,163.00,8.81" xml:id="b41">
	<analytic>
		<title level="a" type="main">Vmeasure: A Conditional Entropy-based External Cluster Evaluation Measure</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="raw_reference">Andrew Rosenberg and Julia Hirschberg. 2007. V- measure: A Conditional Entropy-based External Cluster Evaluation Measure. In EMNLP.</note>
</biblStruct>

<biblStruct coords="11,72.00,608.46,218.27,8.64;11,82.91,619.42,209.02,8.64;11,82.91,630.21,192.31,8.81" xml:id="b42">
	<analytic>
		<title level="a" type="main">Collaborative filtering recommender systems</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Frankowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Herlocker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shilad</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The adaptive web</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="raw_reference">J Ben Schafer, Dan Frankowski, Jon Herlocker, and Shilad Sen. 2007. Collaborative filtering recom- mender systems. In The adaptive web. Springer.</note>
</biblStruct>

<biblStruct coords="11,72.00,649.59,219.92,8.64;11,82.91,660.55,207.53,8.64;11,82.91,671.51,209.10,8.64;11,82.91,682.47,209.02,8.64;11,82.91,693.26,208.60,8.81;11,82.91,704.39,71.67,8.64" xml:id="b43">
	<analytic>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martijn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Schuemie</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Weeber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Bob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erik</forename><forename type="middle">M</forename><surname>Schijvenaars</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Van Mulligen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rob</forename><surname>Christiaan Van Der Eijk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barend</forename><surname>Jelier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Mons</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Distribution of information in biomedical abstracts and full-text publications</title>
				<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2597" to="2604" />
		</imprint>
	</monogr>
	<note type="raw_reference">Martijn J. Schuemie, Marc Weeber, Bob J. A. Schijve- naars, Erik M. van Mulligen, C. Christiaan van der Eijk, Rob Jelier, Barend Mons, and Jan A. Kors. 2004. Distribution of information in biomedical ab- stracts and full-text publications. Bioinformatics, 20(16):2597-604.</note>
</biblStruct>

<biblStruct coords="11,72.00,723.60,218.27,8.64;11,82.91,734.56,207.36,8.64;11,82.91,745.52,209.01,8.64;11,82.91,756.31,71.94,8.81" xml:id="b44">
	<analytic>
		<title level="a" type="main">Improved semantic-aware network embedding with fine-grained word alignment</title>
		<author>
			<persName coords=""><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="raw_reference">Dinghan Shen, Xinyuan Zhang, Ricardo Henao, and Lawrence Carin. 2018. Improved semantic-aware network embedding with fine-grained word align- ment. In EMNLP.</note>
</biblStruct>

<biblStruct coords="11,307.28,67.28,219.92,8.64;11,318.19,78.24,209.10,8.64;11,318.19,89.20,207.36,8.64;11,317.86,99.99,142.76,8.81" xml:id="b45">
	<analytic>
		<title level="a" type="main">An Overview of Microsoft Academic Service (MAS) and Applications</title>
		<author>
			<persName coords=""><forename type="first">Arnab</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Darrin</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo-June Paul</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="raw_reference">Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Dar- rin Eide, Bo-June Paul Hsu, and Kuansan Wang. 2015. An Overview of Microsoft Academic Service (MAS) and Applications. In WWW.</note>
</biblStruct>

<biblStruct coords="11,307.28,122.96,220.01,8.64;11,318.19,133.92,207.53,8.64;11,318.19,144.71,107.89,8.81" xml:id="b46">
	<analytic>
		<title level="a" type="main">Cane: Context-aware network embedding for relation modeling</title>
		<author>
			<persName coords=""><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="raw_reference">Cunchao Tu, Han Liu, Zhiyuan Liu, and Maosong Sun. 2017. Cane: Context-aware network embedding for relation modeling. In ACL.</note>
</biblStruct>

<biblStruct coords="11,307.28,167.68,218.27,8.64;11,318.19,178.64,207.36,8.64;11,318.19,189.60,207.36,8.64;11,317.61,200.39,79.96,8.81" xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. In NIPS.</note>
</biblStruct>

<biblStruct coords="11,307.28,223.36,219.52,8.64;11,318.19,234.32,207.36,8.64;11,317.61,245.28,209.68,8.64;11,318.19,256.24,207.36,8.64;11,318.19,267.03,209.01,8.81;11,318.19,277.99,197.13,8.81" xml:id="b48">
	<analytic>
		<title level="a" type="main">Improving textual network learning with variational homophilic embeddings</title>
		<author>
			<persName coords=""><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liqun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2074" to="2085" />
		</imprint>
	</monogr>
	<note type="raw_reference">Wenlin Wang, Chenyang Tao, Zhe Gan, Guoyin Wang, Liqun Chen, Xinyuan Zhang, Ruiyi Zhang, Qian Yang, Ricardo Henao, and Lawrence Carin. 2019. Improving textual network learning with variational homophilic embeddings. In Advances in Neural In- formation Processing Systems, pages 2074-2085.</note>
</biblStruct>

<biblStruct coords="11,307.28,300.96,220.01,8.64;11,318.19,311.92,209.01,8.64;11,318.19,322.71,209.01,8.81;11,317.83,333.67,20.56,8.58" xml:id="b49">
	<analytic>
		<title level="a" type="main">A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference</title>
		<author>
			<persName coords=""><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="raw_reference">Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A Broad-Coverage Challenge Corpus for Sen- tence Understanding through Inference. In NAACL- HLT.</note>
</biblStruct>

<biblStruct coords="11,307.28,356.64,219.92,8.64;11,318.19,367.60,209.10,8.64;11,318.19,378.56,209.10,8.64;11,318.19,389.35,37.08,8.81" xml:id="b50">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Amauri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianyi</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tao</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="raw_reference">Felix Wu, Amauri H. Souza, Tianyi Zhang, Christo- pher Fifty, Tao Yu, and Kilian Q. Weinberger. 2019a. Simplifying graph convolutional networks. In ICML.</note>
</biblStruct>

<biblStruct coords="11,307.28,412.32,218.27,8.64;11,317.83,423.28,207.72,8.64;11,318.19,434.24,207.36,8.64;11,318.19,445.20,207.36,8.64;11,318.19,455.99,97.95,8.81" xml:id="b51">
	<analytic>
		<title level="a" type="main">Word Mover&apos;s Embedding: From Word2Vec to Document Embedding</title>
		<author>
			<persName coords=""><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>En-Hsu Yen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fangli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Avinash</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Witbrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="raw_reference">Lingfei Wu, Ian En-Hsu Yen, Kun Xu, Fangli Xu, Avinash Balakrishnan, Pin-Yu Chen, Pradeep Ravikumar, and Michael J Witbrock. 2018. Word Mover&apos;s Embedding: From Word2Vec to Document Embedding. In EMNLP.</note>
</biblStruct>

<biblStruct coords="11,307.28,478.96,218.63,8.64;11,318.19,489.92,208.61,8.64;11,318.19,500.88,207.36,8.64;11,318.19,511.84,207.36,8.64;11,318.19,522.80,207.36,8.64;11,318.19,533.59,195.35,8.81" xml:id="b52">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName coords=""><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Macherey</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="raw_reference">Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google&apos;s neural machine translation system: Bridging the gap between human and machine translation. ArXiv, abs/1609.08144.</note>
</biblStruct>

<biblStruct coords="11,307.28,556.56,218.27,8.64;11,318.19,567.52,207.72,8.64;11,318.19,578.48,209.10,8.64;11,317.58,589.27,94.09,8.81" xml:id="b53">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1901.00596</idno>
	</analytic>
	<monogr>
		<title level="j">Comprehensive Survey on Graph Neural Networks. ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="raw_reference">Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. 2019b. A Comprehensive Survey on Graph Neural Networks. ArXiv, abs/1901.00596.</note>
</biblStruct>

<biblStruct coords="11,307.28,612.24,219.92,8.64;11,318.19,623.20,209.10,8.64;11,317.83,634.16,207.89,8.64;11,318.19,644.95,195.07,8.81" xml:id="b54">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
	<note type="raw_reference">Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car- bonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. ArXiv, abs/1906.08237.</note>
</biblStruct>

<biblStruct coords="11,307.28,667.92,219.52,8.64;11,318.19,678.88,209.10,8.64;11,318.19,689.84,209.02,8.64;11,318.19,700.80,207.36,8.64;11,317.86,711.59,27.40,8.58" xml:id="b55">
	<analytic>
		<title level="a" type="main">From neural re-ranking to neural ranking: Learning a sparse representation for inverted indexing</title>
		<author>
			<persName coords=""><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erik</forename><forename type="middle">G</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="raw_reference">Hamed Zamani, Mostafa Dehghani, W. Bruce Croft, Erik G. Learned-Miller, and Jaap Kamps. 2018. From neural re-ranking to neural ranking: Learn- ing a sparse representation for inverted indexing. In CIKM.</note>
</biblStruct>

<biblStruct coords="11,307.28,734.56,218.27,8.64;11,318.19,745.52,207.36,8.64;11,318.19,756.31,132.98,8.81" xml:id="b56">
	<monogr>
		<title level="m" type="main">Diffusion maps for textual network embedding</title>
		<author>
			<persName coords=""><forename type="first">Xinyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
	<note type="raw_reference">Xinyuan Zhang, Yitong Li, Dinghan Shen, and Lawrence Carin. 2018. Diffusion maps for textual network embedding. In NeurIPS.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
