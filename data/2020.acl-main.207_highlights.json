{"@context": ["http://clowder.ncsa.illinois.edu/contexts/metadata.jsonld"], "created_at": "Tue 26 March 14:34:51 UTC 2024", "agent": {"@type": "user", "user_id": "http://clowder.ncsa.illinois.edu/api/users"}, "content": [{"extractor": "ncsa.rctTransparencyExtractor", "extracted_files": [{"file_id": "6602dd09e4b0cd4f641b3900", "filename": "2020.acl-main.207_predicted.csv", "description": "Model predictions in csv file"}, {"file_id": "6602dd0be4b0cd4f641b3913", "filename": "2020.acl-main.207_report.pdf", "description": "RCT report in pdf file"}, {"file_id": "6602dd0be4b0cd4f641b391d", "filename": "2020.acl-main.207_predicted.html", "description": "Model predictions in html file"}], "page_dimensions": {"width": "595.276", "height": "841.89"}, "items_missed": "33", "checklist": [{"section": "Discussion", "missed": "1", "items": [{"topic": "Generalizability", "item": "21", "found": "No", "sentences": []}, {"topic": "Interpretation", "item": "22", "found": "Yes", "sentences": [{"text": "This is because for this task the embeddings are used along with several other informative features in the ranking model (described under task-specific models in \u00a74), meaning that embedding variants have less opportunity for impact on overall performance.", "coords": "6,378.67,411.67,146.88,9.46;6,307.28,425.22,220.08,9.46;6,307.28,438.77,218.27,9.46;6,307.28,452.32,124.32,9.46;6,434.05,450.97,16.43,18.93;6,452.98,452.32,74.38,9.46;6,307.28,465.87,218.27,9.46;6,307.28,479.42,104.18,9.46"}, {"text": "In a dataset of 4,113 clicks, we found that SPECTER ranker improved clickthrough rate over the baseline by 46.5%, demonstrating its superiority.", "coords": "6,392.74,580.54,132.80,9.46;6,307.28,594.09,218.27,9.46;6,307.28,607.64,218.27,9.46;6,307.28,621.19,49.29,9.46"}, {"text": "We emphasize that our citation-based pretraining objective is critical for the performance of SPECTER; removing this and using a vanilla SciB-ERT results in decreased performance on all tasks.", "coords": "6,318.19,641.01,209.17,9.46;6,307.28,654.56,218.27,9.46;6,307.55,668.11,219.81,9.46;6,307.28,681.66,220.10,9.46"}, {"text": "One possible explanation is that author names are sparse in the corpus, making it difficult for the model to infer document-level relatedness from them.", "coords": "7,116.36,519.37,174.10,9.46;7,72.00,532.92,218.27,9.46;7,72.00,546.47,218.27,9.46;7,72.00,560.02,49.41,9.46"}, {"text": "While there could be other potential ways to include hard negatives in the model, our simple approach of including citations of citations is effective.", "coords": "7,447.58,595.35,77.96,9.46;7,307.28,608.90,218.27,9.46;7,307.28,622.45,220.08,9.46;7,307.28,636.00,126.25,9.46"}, {"text": "As illustrated in Table 3, without any additional final task-specific fine-tuning, SPECTER still outperforms a SciBERT model fine-tuned on the end tasks as well as their multitask combination, further demonstrating the effectiveness and versatility of SPECTER embeddings. 15", "coords": "8,439.10,280.19,88.25,9.46;8,307.28,293.74,218.27,9.46;8,307.28,307.29,218.60,9.46;8,307.28,320.83,218.45,9.46;8,307.28,334.38,218.27,9.46;8,307.28,347.93,220.08,9.46;8,307.28,361.48,26.37,9.46;8,333.64,359.44,7.97,6.91"}]}, {"topic": "Limitations", "item": "20", "found": "Yes", "sentences": [{"text": "14 Venue information in our data came directly from publisher provided metadata and thus was not normalized.", "coords": "7,81.66,735.34,5.98,5.18;7,88.14,737.20,203.62,7.77;7,72.00,747.17,193.36,7.77"}]}]}, {"section": "Introduction", "missed": "2", "items": [{"topic": "Background", "item": "2a", "found": "No", "sentences": []}, {"topic": "Objectives", "item": "2b", "found": "No", "sentences": []}]}, {"section": "Methods", "missed": "17", "items": [{"topic": "Blinding", "item": "11a", "found": "No", "sentences": []}, {"topic": "Changes to outcomes", "item": "6b", "found": "No", "sentences": []}, {"topic": "Changes to trial design", "item": "3b", "found": "No", "sentences": []}, {"topic": "Data collection setting", "item": "4b", "found": "No", "sentences": []}, {"topic": "Eligibility criteria", "item": "4a", "found": "No", "sentences": []}, {"topic": "Interim analyses/stopping guidelines", "item": "7b", "found": "No", "sentences": []}, {"topic": "Interventions", "item": "5", "found": "No", "sentences": []}, {"topic": "Outcomes", "item": "6a", "found": "No", "sentences": []}, {"topic": "Randomization - Allocation concealment mechanism", "item": "9", "found": "No", "sentences": []}, {"topic": "Randomization - Implementation", "item": "10", "found": "No", "sentences": []}, {"topic": "Randomization - Sequence generation", "item": "8a", "found": "No", "sentences": []}, {"topic": "Randomization - Type", "item": "8b", "found": "No", "sentences": []}, {"topic": "Sample size determination", "item": "7a", "found": "No", "sentences": []}, {"topic": "Similarity of interventions", "item": "11b", "found": "No", "sentences": []}, {"topic": "Statistical methods for other analyses", "item": "12b", "found": "No", "sentences": []}, {"topic": "Statistical methods for outcomes", "item": "12a", "found": "No", "sentences": []}, {"topic": "Trial design", "item": "3a", "found": "No", "sentences": []}]}, {"section": "Other", "missed": "3", "items": [{"topic": "Funding", "item": "25", "found": "No", "sentences": []}, {"topic": "Protocol access", "item": "24", "found": "No", "sentences": []}, {"topic": "Registration", "item": "23", "found": "No", "sentences": []}]}, {"section": "Results", "missed": "8", "items": [{"topic": "Ancillary analyses", "item": "18", "found": "No", "sentences": []}, {"topic": "Baseline data", "item": "15", "found": "No", "sentences": []}, {"topic": "Binary outcome results", "item": "17b", "found": "No", "sentences": []}, {"topic": "Harms", "item": "19", "found": "No", "sentences": []}, {"topic": "Numbers analyzed", "item": "16", "found": "Yes", "sentences": [{"text": "This process results in about 684K training triples and 145K validation triples.", "coords": "5,327.36,153.24,3.27,9.46"}]}, {"topic": "Outcome results", "item": "17a", "found": "Yes", "sentences": [{"text": "Table 1 presents the main results corresponding to our evaluation tasks (described in \u00a73).", "coords": "6,71.66,552.59,218.61,9.46;6,72.00,566.14,160.43,9.46;6,235.21,564.79,16.90,18.93"}, {"text": "Overall, we observe substantial improvements across all tasks with average performance of 80.0 across all metrics on all tasks which is a 3.1 point absolute improvement over the next-best baseline.", "coords": "6,255.64,566.14,35.99,9.46;6,71.61,579.69,218.66,9.46;6,72.00,593.24,218.27,9.46;6,72.00,606.79,218.27,9.46;6,72.00,620.34,179.73,9.46"}, {"text": "Our evaluation of the learned representations on predicting user activity is shown in the \"User activity\" columns of Table 1.", "coords": "6,72.00,755.86,218.27,9.46;6,307.28,66.67,220.08,9.46;6,307.28,80.22,103.76,9.46"}, {"text": "SPECTER achieves a MAP score of 83.8 on the co-view task, and 84.5 on coread, improving over the best baseline (Citeomatic in this case) by 2.7 and 4.0 points, respectively.", "coords": "6,414.70,80.22,110.59,9.46;6,307.28,93.76,220.08,9.46;6,307.28,107.31,218.27,9.46;6,307.28,120.86,220.17,9.46"}]}, {"topic": "Participant flow", "item": "13a", "found": "No", "sentences": []}, {"topic": "Participant loss/exclusion", "item": "13b", "found": "No", "sentences": []}, {"topic": "Periods of recruitment/follow-up", "item": "14a", "found": "No", "sentences": []}, {"topic": "Trial stopping", "item": "14b", "found": "No", "sentences": []}]}, {"section": "Title and abstract", "missed": "2", "items": [{"topic": "Structured abstract", "item": "1b", "found": "No", "sentences": []}, {"topic": "Title randomized", "item": "1a", "found": "No", "sentences": []}]}]}]}